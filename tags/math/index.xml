<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Math on Tong&#39;Log</title>
    <link>https://zhaotong94.github.io/tags/math/</link>
    <description>Recent content in Math on Tong&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 18 Nov 2024 20:00:57 +0800</lastBuildDate><atom:link href="https://zhaotong94.github.io/tags/math/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Tensor Train for Function Representation</title>
      <link>https://zhaotong94.github.io/blog/ttfr/</link>
      <pubDate>Mon, 18 Nov 2024 20:00:57 +0800</pubDate>
      
      <guid>https://zhaotong94.github.io/blog/ttfr/</guid>
      <description>Considering an arbitrary continuous function $\phi(\mathbf{r})$ on the cube $\mathbf{r} = (x,y,z) \in [0, b]^3$, we approximate $ x $ via a zero-one tuple $(x_1x_2x_3...x_n)\in \{0,1\}^n$ using the following formula: $$ \begin{align*} x \approx b( \frac{x_1}{2^1} + \frac{x_2}{2^2} + \dots + \frac{x_n}{2^n}). \end{align*} $$ Doing the same thing for $y$ and $z$, we can approximately write $\phi(\mathbf{r})$ down in an &amp;ldquo;enumerate&amp;rdquo; form as tensor $$ \begin{align*} \phi(\mathbf{r})\approx\Phi_{x_1x_2...x_ny_1y_2...y_nz_1z_2...z_n}=:\Phi_{\mathbf{r}}, \end{align*} $$ where $\Phi_{\mathbf{r}}$ is a large tensor of $(2^n)^3$ elements. In order to represent the function $\phi(\mathbf{r})$ more accurately, it is necessary to have a fine grid, that is, a sufficiently large $n$. However, an exponentially increasing space complexity is prohibitively high. So, how to reduce the space complexity as much as possible while maintaining a competitive accuracy? For any tensor can be unfolded as a tensor train, more general, tensor network,</description>
    </item>
    
    <item>
      <title>Connections among Over-determination, Under-Determination, Least-Squares, Pseudo-Inverse, and SVD Decomposition.</title>
      <link>https://zhaotong94.github.io/blog/codudlspisvd/</link>
      <pubDate>Mon, 18 Nov 2024 17:38:00 +0800</pubDate>
      
      <guid>https://zhaotong94.github.io/blog/codudlspisvd/</guid>
      <description>Considering a linear equation $$ \begin{align} Ax=b,\label{le} \end{align} $$ where $A\in \mathbb{R}^{n\times m}, x\in\mathbb{R}^{m}$ and $b\in\mathbb{R}^{n}$. If $m &lt; n$, we call the equation over-determinated. Else if $m&gt;n$, we call the equation under-determinated. For over-determinated system, it usually does not have a solution, if $b$ lies out of the space spaned by column vectors $a_i$ of $A$. Therefore, to solve the problem, we first project both side of the equation into the space spaned by $a_i$. So, we do $$ \begin{align} A^{\textsf{T}}Ax=A^{\textsf{T}}b.\label{pls} \end{align} $$ If $A^{\textsf{T}}A$ is invertible, we have $$ \begin{align*} x=(A^{\textsf{T}}A)^{-1}A^{\textsf{T}}b. \end{align*} $$ We call $(A^{\textsf{T}}A)^{-1}A^{\textsf{T}}$ Moore-Penrose Pseudoinverse. On least squares perspective, it is equavalent to the following problem $$ \begin{align*} \min_x f(x)=\|Ax-b\|^2. \end{align*} $$ Via setting $\nabla f=0$, we also have equation \eqref{pls}. If $A^{\textsf{T}}A$ is not invertible, we will treat it in the same way as</description>
    </item>
    
    <item>
      <title>How to Create a Small and Smooth Dynamic GIF for Dynamical Systems?</title>
      <link>https://zhaotong94.github.io/blog/hcssdgds/</link>
      <pubDate>Wed, 23 Oct 2024 14:05:34 +0800</pubDate>
      
      <guid>https://zhaotong94.github.io/blog/hcssdgds/</guid>
      <description>When giving a presentation on STEM disciplines, we usually need to draw a gif of some dynamical systems to express our ideas clearly. However, creating a portable, fluid, and authentic gif is challenging, it is very hard to find suitable parameters for drawing a gif.</description>
    </item>
    
    <item>
      <title>Reproducing Kernel Hilbert Space</title>
      <link>https://zhaotong94.github.io/blog/rkhs/</link>
      <pubDate>Thu, 05 May 2022 00:18:23 +0800</pubDate>
      
      <guid>https://zhaotong94.github.io/blog/rkhs/</guid>
      <description>A brief introduction to reproducing kernel hilbert Space.</description>
    </item>
    
  </channel>
</rss>
