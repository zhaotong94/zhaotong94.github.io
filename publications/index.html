<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/0.4.2/html2canvas.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jspdf/2.5.1/jspdf.umd.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jspdf-autotable/3.5.24/jspdf.plugin.autotable.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.9.2/html2pdf.bundle.js"></script>
    


    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="/css/bootstrap.min.css">
    <link rel="stylesheet" href="/css/bootstrap-toc.min.css">
    <link rel="stylesheet" href="/css/mdb.min.css">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/5.0.0-beta1/js/bootstrap.bundle.min.js"></script>
    <script src="/js/bootstrap.bundle.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.bundle.min.js"></script>
    

    <script src="/js/theme.js"></script>
    <script src="/js/common.js"></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>üìÑPublications | Tong&#39;Log</title>
<meta name="keywords" content="">
<meta name="description" content="@inproceedings{hu2023rlekf, abbr={AAAI}, title={RLEKF: An optimizer for deep potential with ab initio accuracy}, author={Hu*, Siyu and Zhang*, Wentao and Sha, Qiuchen and Pan, Feng and Wang, Lin-Wang and Jia, Weile and Tan, Guangming and Zhao‚Ä†, Tong}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, volume={37}, number={7}, pages={7910‚Äì7918}, year={2023}, abstract={It is imperative to accelerate the training of neural network force field such as Deep Potential, which usually requires thousands of images based on first-principles calculation and a couple of days to generate an accurate potential energy surface. To this end, we propose a novel optimizer named reorganized layer extended Kalman filtering (RLEKF), an optimized version of global extended Kalman filtering (GEKF) with a strategy of splitting big and gathering small">
<meta name="author" content="Tong">
<link rel="canonical" href="https://zhaotong94.github.io/publications/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://zhaotong94.github.io/img/capybara.jpg">
<link rel="icon" type="image/png" sizes="16x16" href="https://zhaotong94.github.io/img/capybara.jpg">
<link rel="icon" type="image/png" sizes="32x32" href="https://zhaotong94.github.io/img/capybara.jpg">
<link rel="apple-touch-icon" href="https://zhaotong94.github.io/img/capybara.jpg">
<link rel="mask-icon" href="https://zhaotong94.github.io/img/capybara.jpg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://zhaotong94.github.io/publications/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>




<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script type="text/javascript">
    MathJax = {
      tex: {
        tags:"ams",
        inlineMath: [['$', '$']],
        displayMath: [['$$','$$']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  
    
    
    
    
  
  </script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
<script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js"></script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = ""; 
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

  

<meta property="og:title" content="üìÑPublications" />
<meta property="og:description" content="@inproceedings{hu2023rlekf, abbr={AAAI}, title={RLEKF: An optimizer for deep potential with ab initio accuracy}, author={Hu*, Siyu and Zhang*, Wentao and Sha, Qiuchen and Pan, Feng and Wang, Lin-Wang and Jia, Weile and Tan, Guangming and Zhao‚Ä†, Tong}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, volume={37}, number={7}, pages={7910‚Äì7918}, year={2023}, abstract={It is imperative to accelerate the training of neural network force field such as Deep Potential, which usually requires thousands of images based on first-principles calculation and a couple of days to generate an accurate potential energy surface. To this end, we propose a novel optimizer named reorganized layer extended Kalman filtering (RLEKF), an optimized version of global extended Kalman filtering (GEKF) with a strategy of splitting big and gathering small" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://zhaotong94.github.io/publications/" /><meta property="article:section" content="" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="üìÑPublications"/>
<meta name="twitter:description" content="@inproceedings{hu2023rlekf, abbr={AAAI}, title={RLEKF: An optimizer for deep potential with ab initio accuracy}, author={Hu*, Siyu and Zhang*, Wentao and Sha, Qiuchen and Pan, Feng and Wang, Lin-Wang and Jia, Weile and Tan, Guangming and Zhao‚Ä†, Tong}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, volume={37}, number={7}, pages={7910‚Äì7918}, year={2023}, abstract={It is imperative to accelerate the training of neural network force field such as Deep Potential, which usually requires thousands of images based on first-principles calculation and a couple of days to generate an accurate potential energy surface. To this end, we propose a novel optimizer named reorganized layer extended Kalman filtering (RLEKF), an optimized version of global extended Kalman filtering (GEKF) with a strategy of splitting big and gathering small"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "üìÑPublications",
      "item": "https://zhaotong94.github.io/publications/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "üìÑPublications",
  "name": "üìÑPublications",
  "description": "@inproceedings{hu2023rlekf, abbr={AAAI}, title={RLEKF: An optimizer for deep potential with ab initio accuracy}, author={Hu*, Siyu and Zhang*, Wentao and Sha, Qiuchen and Pan, Feng and Wang, Lin-Wang and Jia, Weile and Tan, Guangming and Zhao‚Ä†, Tong}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, volume={37}, number={7}, pages={7910‚Äì7918}, year={2023}, abstract={It is imperative to accelerate the training of neural network force field such as Deep Potential, which usually requires thousands of images based on first-principles calculation and a couple of days to generate an accurate potential energy surface. To this end, we propose a novel optimizer named reorganized layer extended Kalman filtering (RLEKF), an optimized version of global extended Kalman filtering (GEKF) with a strategy of splitting big and gathering small",
  "keywords": [
    
  ],
  "articleBody": "@inproceedings{hu2023rlekf, abbr={AAAI}, title={RLEKF: An optimizer for deep potential with ab initio accuracy}, author={Hu*, Siyu and Zhang*, Wentao and Sha, Qiuchen and Pan, Feng and Wang, Lin-Wang and Jia, Weile and Tan, Guangming and Zhao‚Ä†, Tong}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, volume={37}, number={7}, pages={7910‚Äì7918}, year={2023}, abstract={It is imperative to accelerate the training of neural network force field such as Deep Potential, which usually requires thousands of images based on first-principles calculation and a couple of days to generate an accurate potential energy surface. To this end, we propose a novel optimizer named reorganized layer extended Kalman filtering (RLEKF), an optimized version of global extended Kalman filtering (GEKF) with a strategy of splitting big and gathering small layers to overcome the $O(N^2)$ computational cost of GEKF. This strategy provides an approximation of the dense weights error covariance matrix with a sparse diagonal block matrix for GEKF. We implement both RLEKF and the baseline Adam in our Œ±Dynamics package and numerical experiments are performed on 13 unbiased datasets. Overall, RLEKF converges faster with slightly better accuracy. For example, a test on a typical system, bulk copper, shows that RLEKF converges faster by both the number of training epochs (√ó11.67) and wall-clock time (√ó1.19). Besides, we theoretically prove that the updates of weights converge and thus are against the gradient exploding problem. Experimental results verify that RLEKF is not sensitive to the initialization of weights. The RLEKF sheds light on other AI-for-science applications where training a large neural network (with tons of thousands parameters) is a bottleneck.}, bibtex_show={true}, pdf={https://arxiv.org/pdf/2212.06989}, code={}, blog={}, poster={}, slides={}, video={}, url={}, grade={oral}, doi={https://doi.org/10.1609/aaai.v37i7.25957}, altmetric={248277}, dimensions={true}, google_scholar_id={}, selected={true}, publisher={}, }\n@inproceedings{feng2024accelerating, abbr={EuroPar}, bibtex_show={true}, title={Accelerating large-scale sparse LU factorization for RF circuit simulation}, abstract={Sparse LU factorization is the indispensable building block of the circuit simulation, and dominates the simulation time, especially when dealing with large-scale circuits. Radio frequency (RF) circuits have been increasingly emphasized with the evolution of ubiquitous wireless communication (i.e., 5G and WiFi). The RF simulation matrices show a distinctive pattern of structured dense blocks, and this pattern has been inadvertently overlooked by prior works, leading to the underutilization of computational resources. In this paper, by exploiting the block structure, we propose a novel blocked format for L and U factors and re-design the large-scale sparse LU factorization accordingly, which leverages the data locality inherent in RF matrices. The data format transformation is streamlined, strategically eliminating the redundant data movement and costly indirect memory access. Moreover, the vector operations are converted into matrix operations, enabling efficient data reuse and enhancing data-level parallelism. The experiment results demonstrate that our method achieves superior performance compared to state-of-the-art implementation.}, author={Feng, Guofeng and Wang, Hongyu and Guo, Zhuoqiang and Li‚Ä†, Mingzhen and Zhao, Tong and Jin, Zhou and Jia, Weile and Tan, Guangming and Sun, Ninghui}, booktitle={European Conference on Parallel Processing}, volume={}, number={}, pages={182‚Äì195}, year={2024}, publisher={Springer}, doi={https://doi.org/10.1007/978-3-031-69583-4_13}, }\n@inproceedings{hu2024training, abbr={PPoPP}, bibtex_show={true}, title={Training one deepmd model in minutes: A step towards online learning}, abstract={Neural Network Molecular Dynamics (NNMD) has become a major approach in material simulations, which can speedup the molecular dynamics (MD) simulation for thousands of times, while maintaining ab initio accuracy, thus has a potential to fundamentally change the paradigm of material simulations. However, there are two time-consuming bottlenecks of the NNMD developments. One is the data access of ab initio calculation results. The other, which is the focus of the current work, is reducing the training time of NNMD model. The training of NNMD model is different from most other neural network training because the atomic force (which is related to the gradient of the network) is an important physical property to be fit. Tests show the traditional stochastic gradient methods, like the Adam algorithms, cannot efficiently deploy the multisample minibatch algorithm. As a result, a typical training (taking the Deep Potential Molecular Dynamics (DeePMD) as an example) can take many hours. In this work, we designed a heuristic minibatch quasi-Newtonian optimizer based on Extended Kalman Filter method. An early reduction of gradient and error is adopted to reduce memory footprint and communication. The memory footprint, communication and settings of hyper-parameters of this new method are analyzed in detail. Computational innovations such as customized kernels of the symmetry-preserving descriptor are applied to exploit the computing power of the heterogeneous architecture. Experiments are performed on 8 different datasets representing different real case situations, and numerical results show that our new method has an average speedup of 32.2 compared to the Reorganized Layer-wised Extended Kalman Filter with 1 GPU, reducing the absolute training time of one DeePMD model from hours to several minutes, making it one step toward online training.}, author={Hu, Siyu and Zhao, Tong and Sha, Qiuchen and Li, Enji and Meng, Xiangyu and Liu, Lijun and Wang, Lin-Wang and Tan, Guangming and Jia, Weile}, booktitle={Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming}, pdf={https://dl.acm.org/doi/pdf/10.1145/3627535.3638505}, selected={true}, volume={}, number={}, pages={257‚Äì269}, year={2024}, publisher={}, }\n@inproceedings{li1950enhance, abbr={SC}, bibtex_show={true}, title={Enhance the strong scaling of lammps on fugaku}, abstract={Physical phenomenon such as protein folding requires simulation up to microseconds of physical time, which directly corresponds to the strong scaling of molecular dynamics(MD) on modern supercomputers. In this paper, we present a highly scalable implementation of the state-of-the-art MD code LAMMPS on Fugaku by exploiting the 6D mesh/torus topology of the TofuD network. Based on our detailed analysis of the MD communication pattern, we first adapt coarse-grained peer-to-peer ghost-region communication with uTofu interface, then further improve the scalability via fine-grained thread pool. Finally, Remote direct memory access (RDMA) primitives are utilized to avoid buffer overhead. Numerical results show that our optimized code can reduce 77% of the communication time, improving the performance of baseline LAMMPS by a factor of 2.9x and 2.2x for Lennard-Jones and embedded-atom method potentials when scaling to 36, 846 computing nodes. Our optimization techniques can also benefit other applications with stencil or domain decomposition methods.}, author={Li, Jianxiong and Zhao, Tong and Guo, Zuoqiang and Shi, Shunchen and Liu, Lijun and Tan, Guangming and Jia‚Ä†, Weile and Yuan, Guojun and Wang‚Ä†, Zhan}, booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, pdf={https://dl.acm.org/doi/pdf/10.1145/3581784.3607064}, volume={}, number={}, pages={1-13}, year={2023}, publisher={Association for Computing Machinery}, }\n@article{yan190510-million, bibtex_show={true}, abbr={JCST}, title={10-million atoms simulation of first-principle package LS3DF}, author={Yan, Yujin and Li‚Ä†, HaiBo and Zhao, Tong and Wang, Lin-Wang and Shi, Lin and Liu, Tao and Tan, GuangMing and Jia‚Ä†, Weile and Sun‚Ä†, Ninghui}, abstract={The growing demand for semiconductor devices simulation poses a big challenge for large-scale electronic structure calculations. Among various methods, the linearly scaling three-dimensional fragment (LS3DF) method exhibits excellent scalability in large-scale simulations. Based on algorithmic and system-level optimizations, we propose a highly scalable and highly efficient implementation of LS3DF on a domestic heterogeneous supercomputer equipped with accelerators. In terms of algorithmic optimizations, the original all-band conjugate gradient algorithm is refined to achieve faster convergence, and mixed precision computing is adopted to increase overall efficiency. In terms of system-level optimizations, the original two-layer parallel structure is replaced by a coarse-grained parallel method. Optimization strategies such as multi-stream, kernel fusion, and redundant computation removal are proposed to increase further utilization of the computational power provided by the heterogeneous machines. As a result, our optimized LS3DF can scale to a 10-million silicon atoms system, attaining a peak performance of 34.8 PFLOPS (21.2% of the peak). All the improvements can be adapted to the next-generation supercomputers for larger simulations.}, journal={Journal of Computer Science and Technology}, volume={39}, number={1}, pages={45‚Äì62}, year={2024}, doi={https://doi.org/10.1007/s11390-023-3011-6}, award={}, award_name={}, }\n@article{zhao2022limits, bibtex_show={true}, abbr={CAM}, title={Limits of one-dimensional interacting particle systems with two-scale interaction}, author={Zhao, Tong}, abstract={This paper characterizes the limits of a large system of interacting particles distributed on the real line. The interaction occurring among neighbors involves two kinds of independent actions with different rates. This system is a generalization of the voter process, of which each particle is of type A or a. Under suitable scaling, the local proportion functions of A particles converge to continuous functions which solve a class of stochastic partial differential equations driven by Fisher-Wright white noise. To obtain the convergence, the tightness of these functions is derived from the moment estimate method.}, journal={Chinese Annals of Mathematics, Series B}, volume={43}, number={2}, pages={195-208}, year={2022}, doi={https://doi.org/10.1007/s11401-022-0311-z}, selected={true}, award={}, award_name={}, }\n",
  "wordCount" : "1476",
  "inLanguage": "en",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Tong"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://zhaotong94.github.io/publications/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Tong'Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://zhaotong94.github.io/img/capybara.jpg"
    }
  }
}
</script>
    
    
    
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://zhaotong94.github.io/" accesskey="h" title="Tong&#39;Log (Alt + H)">
            <img src="https://zhaotong94.github.io/img/capybara.jpg" alt="logo" aria-label="logo"
                 height="35">Tong&#39;Log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://zhaotong94.github.io/" title="üè†Home">
                <span>üè†Home</span>
                </a>
            </li>
            <li>
                <a href="https://zhaotong94.github.io/blog" title="‚úçBlog">
                <span>‚úçBlog</span>
                </a>
            </li>
            <li>
                <a href="https://zhaotong94.github.io/publications" title="üìÑPublications">
                <span class="active">üìÑPublications</span>
                </a>
            </li>
            <li>
                <a href="https://zhaotong94.github.io/tags" title="üè∑Ô∏èTags">
                <span>üè∑Ô∏èTags</span>
                </a>
            </li>
            <li>
                <a href="https://zhaotong94.github.io/cv" title="üë®‚Äçü¶±CV">
                <span>üë®‚Äçü¶±CV</span>
                </a>
            </li>
            <li>
                <a href="https://zhaotong94.github.io/faq" title="ü§îFAQ">
                <span>ü§îFAQ</span>
                </a>
            </li>
            <li>
                <a href="https://zhaotong94.github.io/search" title="üîçSearch (Alt &#43; /)" accesskey=/>
                <span>üîçSearch</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">


<h1>üìÑPublications</h1>
(*) denotes equal contribution, (‚Ä†) denotes corresponding author



<div class="publications">




    
        <h2 class="bibliography">2024</h2>
        
    
<ol class="bibliography"><li>
    
<div class="row"> 
     
    
        <div class="col col-sm-1 abbr" style="text-align: center">
            <abbr class="badge rounded w-100" style="background-color:#FF6C0C">JCST</abbr>
            
        
        </div>
    

    
    
    <div id="yan190510-million" class="col-sm-11">
        
        <div class="title">10-million atoms simulation of first-principle package LS3DF</div>
        
        <div class="author">
            
            
            

            

            
            
            
            
            

            
            

            
            
            

            
                
                
                    Yujin&nbspYan,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    HaiBo&nbspLi<sup>‚Ä†</sup>,
                
                
            
            
            
            
            
            

            
            
                
                
                
            

            
            
            

            
                
                <em>Tong&nbspZhao</em>,
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Lin-Wang&nbspWang,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Lin&nbspShi,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Tao&nbspLiu,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    GuangMing&nbspTan,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Weile&nbspJia<sup>‚Ä†</sup>,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    and Ninghui&nbspSun<sup>‚Ä†</sup>
                
                
            
            

            
            
            

            
        </div>

        
        
        
         

        
            
        

        
        

        
        
            
        

        
        

        

         
        
        
          
            
        
        <div class="periodical">
            <em>Journal of Computer Science and Technology</em>,  2024
        </div>
        <div class="periodical">
            
        </div>
        

        
        <div class="links">
            
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            <a href="https://doi.org/https://doi.org/10.1007/s11390-023-3011-6" class="btn btn-sm z-depth-0" role="button">DOI</a>
            
            
            
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            
            
            
            
            
            
            
            
            
            
        </div>

        

        
            
            <div class="abstract hidden">
            <p>The growing demand for semiconductor devices simulation poses a big challenge for large-scale electronic structure calculations. Among various methods, the linearly scaling three-dimensional fragment (LS3DF) method exhibits excellent scalability in large-scale simulations. Based on algorithmic and system-level optimizations, we propose a highly scalable and highly efficient implementation of LS3DF on a domestic heterogeneous supercomputer equipped with accelerators. In terms of algorithmic optimizations, the original all-band conjugate gradient algorithm is refined to achieve faster convergence, and mixed precision computing is adopted to increase overall efficiency. In terms of system-level optimizations, the original two-layer parallel structure is replaced by a coarse-grained parallel method. Optimization strategies such as multi-stream, kernel fusion, and redundant computation removal are proposed to increase further utilization of the computational power provided by the heterogeneous machines. As a result, our optimized LS3DF can scale to a 10-million silicon atoms system, attaining a peak performance of 34.8 PFLOPS (21.2% of the peak). All the improvements can be adapted to the next-generation supercomputers for larger simulations.</p>
            </div>
        

        
            
            <div class="bibtex hidden">
            <div class="post-content highlight"><pre><code class="bibtex">@article&#123yan202410million,
            title   = "10-million atoms simulation of first-principle package LS3DF",
            author  = "Yujin Yan and HaiBo Li and Tong Zhao and Lin-Wang Wang and Lin Shi and Tao Liu and GuangMing Tan and Weile Jia and Ninghui Sun",
            year    = "2024",<br/>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;journal = "Journal of Computer Science and Technology",<br/>&#125
            </code></pre></div>
            </div>
        

            
    </div>  
</div> 
</li></ol> 

    
<ol class="bibliography"><li>
    
<div class="row"> 
     
    
        <div class="col col-sm-1 abbr" style="text-align: center">
            <abbr class="badge rounded w-100" style="background-color:#FF6C0C">PPoPP</abbr>
            
        
        </div>
    

    
    
    <div id="hu2024training" class="col-sm-11">
        
        <div class="title">Training one deepmd model in minutes: A step towards online learning</div>
        
        <div class="author">
            
            
            

            

            
            
            
            
            

            
            

            
            
            

            
                
                
                    Siyu&nbspHu,
                
                
            
            
            
            
            
            

            
            
                
                
                
            

            
            
            

            
                
                <em>Tong&nbspZhao</em>,
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Qiuchen&nbspSha,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Enji&nbspLi,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Xiangyu&nbspMeng,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Lijun&nbspLiu,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Lin-Wang&nbspWang,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Guangming&nbspTan,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    and Weile&nbspJia
                
                
            
            

            
            
            

            
        </div>

        
        
        
         

        
            
        

        
        

        
        
            
        

        
        

        

         
        
        
          
            
        
        <div class="periodical">
            <em>In Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming</em>,  2024
        </div>
        <div class="periodical">
            
        </div>
        

        
        <div class="links">
            
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            
            
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            
            
            
            
                <a href="https://dl.acm.org/doi/pdf/10.1145/3627535.3638505" class="btn btn-sm z-depth-0" role="button">PDF</a>
            
            
            
            
            
            
            
            
            
        </div>

        

        
            
            <div class="abstract hidden">
            <p>Neural Network Molecular Dynamics (NNMD) has become a major approach in material simulations, which can speedup the molecular dynamics (MD) simulation for thousands of times, while maintaining ab initio accuracy, thus has a potential to fundamentally change the paradigm of material simulations. However, there are two time-consuming bottlenecks of the NNMD developments. One is the data access of ab initio calculation results. The other, which is the focus of the current work, is reducing the training time of NNMD model. The training of NNMD model is different from most other neural network training because the atomic force (which is related to the gradient of the network) is an important physical property to be fit. Tests show the traditional stochastic gradient methods, like the Adam algorithms, cannot efficiently deploy the multisample minibatch algorithm. As a result, a typical training (taking the Deep Potential Molecular Dynamics (DeePMD) as an example) can take many hours. In this work, we designed a heuristic minibatch quasi-Newtonian optimizer based on Extended Kalman Filter method. An early reduction of gradient and error is adopted to reduce memory footprint and communication. The memory footprint, communication and settings of hyper-parameters of this new method are analyzed in detail. Computational innovations such as customized kernels of the symmetry-preserving descriptor are applied to exploit the computing power of the heterogeneous architecture. Experiments are performed on 8 different datasets representing different real case situations, and numerical results show that our new method has an average speedup of 32.2 compared to the Reorganized Layer-wised Extended Kalman Filter with 1 GPU, reducing the absolute training time of one DeePMD model from hours to several minutes, making it one step toward online training.</p>
            </div>
        

        
            
            <div class="bibtex hidden">
            <div class="post-content highlight"><pre><code class="bibtex">@article&#123hu2024training,
            title   = "Training one deepmd model in minutes: A step towards online learning",
            author  = "Siyu Hu and Tong Zhao and Qiuchen Sha and Enji Li and Xiangyu Meng and Lijun Liu and Lin-Wang Wang and Guangming Tan and Weile Jia",
            year    = "2024",<br/>&#125
            </code></pre></div>
            </div>
        

            
    </div>  
</div> 
</li></ol> 

    
<ol class="bibliography"><li>
    
<div class="row"> 
     
    
        <div class="col col-sm-1 abbr" style="text-align: center">
            <abbr class="badge rounded w-100" style="background-color:#FF6C0C">EuroPar</abbr>
            
        
        </div>
    

    
    
    <div id="feng2024accelerating" class="col-sm-11">
        
        <div class="title">Accelerating large-scale sparse LU factorization for RF circuit simulation</div>
        
        <div class="author">
            
            
            

            

            
            
            
            
            

            
            

            
            
            

            
                
                
                    Guofeng&nbspFeng,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Hongyu&nbspWang,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Zhuoqiang&nbspGuo,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Mingzhen&nbspLi<sup>‚Ä†</sup>,
                
                
            
            
            
            
            
            

            
            
                
                
                
            

            
            
            

            
                
                <em>Tong&nbspZhao</em>,
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Zhou&nbspJin,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Weile&nbspJia,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Guangming&nbspTan,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    and Ninghui&nbspSun
                
                
            
            

            
            
            

            
        </div>

        
        
        
         

        
            
        

        
        

        
        
            
        

        
        

        

         
        
        
          
            
        
        <div class="periodical">
            <em>In European Conference on Parallel Processing</em>,  2024
        </div>
        <div class="periodical">
            
        </div>
        

        
        <div class="links">
            
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            <a href="https://doi.org/https://doi.org/10.1007/978-3-031-69583-4_13" class="btn btn-sm z-depth-0" role="button">DOI</a>
            
            
            
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            
            
            
            
            
            
            
            
            
            
        </div>

        

        
            
            <div class="abstract hidden">
            <p>Sparse LU factorization is the indispensable building block of the circuit simulation, and dominates the simulation time, especially when dealing with large-scale circuits. Radio frequency (RF) circuits have been increasingly emphasized with the evolution of ubiquitous wireless communication (i.e., 5G and WiFi). The RF simulation matrices show a distinctive pattern of structured dense blocks, and this pattern has been inadvertently overlooked by prior works, leading to the underutilization of computational resources. In this paper, by exploiting the block structure, we propose a novel blocked format for L and U factors and re-design the large-scale sparse LU factorization accordingly, which leverages the data locality inherent in RF matrices. The data format transformation is streamlined, strategically eliminating the redundant data movement and costly indirect memory access. Moreover, the vector operations are converted into matrix operations, enabling efficient data reuse and enhancing data-level parallelism. The experiment results demonstrate that our method achieves superior performance compared to state-of-the-art implementation.</p>
            </div>
        

        
            
            <div class="bibtex hidden">
            <div class="post-content highlight"><pre><code class="bibtex">@article&#123feng2024accelerating,
            title   = "Accelerating large-scale sparse LU factorization for RF circuit simulation",
            author  = "Guofeng Feng and Hongyu Wang and Zhuoqiang Guo and Mingzhen Li and Tong Zhao and Zhou Jin and Weile Jia and Guangming Tan and Ninghui Sun",
            year    = "2024",<br/>&#125
            </code></pre></div>
            </div>
        

            
    </div>  
</div> 
</li></ol> 

    
        <h2 class="bibliography">2023</h2>
        
    
<ol class="bibliography"><li>
    
<div class="row"> 
     
    
        <div class="col col-sm-1 abbr" style="text-align: center">
            <abbr class="badge rounded w-100" style="background-color:#FF6C0C">SC</abbr>
            
        
        </div>
    

    
    
    <div id="li1950enhance" class="col-sm-11">
        
        <div class="title">Enhance the strong scaling of lammps on fugaku</div>
        
        <div class="author">
            
            
            

            

            
            
            
            
            

            
            

            
            
            

            
                
                
                    Jianxiong&nbspLi,
                
                
            
            
            
            
            
            

            
            
                
                
                
            

            
            
            

            
                
                <em>Tong&nbspZhao</em>,
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Zuoqiang&nbspGuo,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Shunchen&nbspShi,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Lijun&nbspLiu,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Guangming&nbspTan,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Weile&nbspJia<sup>‚Ä†</sup>,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Guojun&nbspYuan,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    and Zhan&nbspWang<sup>‚Ä†</sup>
                
                
            
            

            
            
            

            
        </div>

        
        
        
         

        
            
        

        
        

        
        
            
        

        
        

        

         
        
        
          
            
        
        <div class="periodical">
            <em>In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</em>,  2023
        </div>
        <div class="periodical">
            
        </div>
        

        
        <div class="links">
            
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            
            
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            
            
            
            
                <a href="https://dl.acm.org/doi/pdf/10.1145/3581784.3607064" class="btn btn-sm z-depth-0" role="button">PDF</a>
            
            
            
            
            
            
            
            
            
        </div>

        

        
            
            <div class="abstract hidden">
            <p>Physical phenomenon such as protein folding requires simulation up to microseconds of physical time, which directly corresponds to the strong scaling of molecular dynamics(MD) on modern supercomputers. In this paper, we present a highly scalable implementation of the state-of-the-art MD code LAMMPS on Fugaku by exploiting the 6D mesh/torus topology of the TofuD network. Based on our detailed analysis of the MD communication pattern, we first adapt coarse-grained peer-to-peer ghost-region communication with uTofu interface, then further improve the scalability via fine-grained thread pool. Finally, Remote direct memory access (RDMA) primitives are utilized to avoid buffer overhead. Numerical results show that our optimized code can reduce 77% of the communication time, improving the performance of baseline LAMMPS by a factor of 2.9x and 2.2x for Lennard-Jones and embedded-atom method potentials when scaling to 36, 846 computing nodes. Our optimization techniques can also benefit other applications with stencil or domain decomposition methods.</p>
            </div>
        

        
            
            <div class="bibtex hidden">
            <div class="post-content highlight"><pre><code class="bibtex">@article&#123li2023enhance,
            title   = "Enhance the strong scaling of lammps on fugaku",
            author  = "Jianxiong Li and Tong Zhao and Zuoqiang Guo and Shunchen Shi and Lijun Liu and Guangming Tan and Weile Jia and Guojun Yuan and Zhan Wang",
            year    = "2023",<br/>&#125
            </code></pre></div>
            </div>
        

            
    </div>  
</div> 
</li></ol> 

    
<ol class="bibliography"><li>
    
<div class="row"> 
     
    
        <div class="col col-sm-1 abbr" style="text-align: center">
            <abbr class="badge rounded w-100" style="background-color:#FF6C0C">AAAI</abbr>
            
        
            <abbr class="board rounded w-100">Oral</abbr>
            
        
        </div>
    

    
    
    <div id="hu2023rlekf" class="col-sm-11">
        
        <div class="title">RLEKF: An optimizer for deep potential with ab initio accuracy</div>
        
        <div class="author">
            
            
            

            

            
            
            
            
            

            
            

            
            
            

            
                
                
                    Siyu&nbspHu<sup>*</sup>,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Wentao&nbspZhang<sup>*</sup>,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Qiuchen&nbspSha,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Feng&nbspPan,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Lin-Wang&nbspWang,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Weile&nbspJia,
                
                
            
            
            
            
            
            

            
            

            
            
            

            
                
                
                    Guangming&nbspTan,
                
                
            
            
            
            
            
            

            
            
                
                
                
            

            
            
            

            
                
                and <em>Tong&nbspZhao<sup>‚Ä†</sup></em>
                
            
            

            
            
            

            
        </div>

        
        
        
         

        
            
        

        
        

        
        
            
        

        
        

        

         
        
        
          
            
        
        <div class="periodical">
            <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em>,  2023
        </div>
        <div class="periodical">
            
        </div>
        

        
        <div class="links">
            
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            <a href="https://doi.org/https://doi.org/10.1609/aaai.v37i7.25957" class="btn btn-sm z-depth-0" role="button">DOI</a>
            
            
            
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            
            
            
            
                <a href="https://arxiv.org/pdf/2212.06989" class="btn btn-sm z-depth-0" role="button">PDF</a>
            
            
            
            
            
            
            
            
            
        </div>

        

        
            
            <div class="abstract hidden">
            <p>It is imperative to accelerate the training of neural network force field such as Deep Potential, which usually requires thousands of images based on first-principles calculation and a couple of days to generate an accurate potential energy surface. To this end, we propose a novel optimizer named reorganized layer extended Kalman filtering (RLEKF), an optimized version of global extended Kalman filtering (GEKF) with a strategy of splitting big and gathering small layers to overcome the $O(N^2)$ computational cost of GEKF. This strategy provides an approximation of the dense weights error covariance matrix with a sparse diagonal block matrix for GEKF. We implement both RLEKF and the baseline Adam in our Œ±Dynamics package and numerical experiments are performed on 13 unbiased datasets. Overall, RLEKF converges faster with slightly better accuracy. For example, a test on a typical system, bulk copper, shows that RLEKF converges faster by both the number of training epochs (√ó11.67) and wall-clock time (√ó1.19). Besides, we theoretically prove that the updates of weights converge and thus are against the gradient exploding problem. Experimental results verify that RLEKF is not sensitive to the initialization of weights. The RLEKF sheds light on other AI-for-science applications where training a large neural network (with tons of thousands parameters) is a bottleneck.</p>
            </div>
        

        
            
            <div class="bibtex hidden">
            <div class="post-content highlight"><pre><code class="bibtex">@article&#123hu2023rlekf,
            title   = "RLEKF: An optimizer for deep potential with ab initio accuracy",
            author  = "Siyu Hu and Wentao Zhang and Qiuchen Sha and Feng Pan and Lin-Wang Wang and Weile Jia and Guangming Tan and Tong Zhao",
            year    = "2023",<br/>&#125
            </code></pre></div>
            </div>
        

            
    </div>  
</div> 
</li></ol> 

    
        <h2 class="bibliography">2022</h2>
        
    
<ol class="bibliography"><li>
    
<div class="row"> 
     
    
        <div class="col col-sm-1 abbr" style="text-align: center">
            <abbr class="badge rounded w-100" style="background-color:#FF6C0C">CAM</abbr>
            
        
        </div>
    

    
    
    <div id="zhao2022limits" class="col-sm-11">
        
        <div class="title">Limits of one-dimensional interacting particle systems with two-scale interaction</div>
        
        <div class="author">
            
            
            

            

            
            
            
            
            

            
            
                
                
                
            

            
            
            

            
                <em>Tong&nbspZhao</em>
            
            

            
            
            

            
        </div>

        
        
        
         

        
            
        

        
        

        
        
            
        

        
        

        

         
        
        
          
            
        
        <div class="periodical">
            <em>Chinese Annals of Mathematics, Series B</em>,  2022
        </div>
        <div class="periodical">
            
        </div>
        

        
        <div class="links">
            
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            
            
            <a href="https://doi.org/https://doi.org/10.1007/s11401-022-0311-z" class="btn btn-sm z-depth-0" role="button">DOI</a>
            
            
            
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            
            
            
            
            
            
            
            
            
            
        </div>

        

        
            
            <div class="abstract hidden">
            <p>This paper characterizes the limits of a large system of interacting particles distributed on the real line. The interaction occurring among neighbors involves two kinds of independent actions with different rates. This system is a generalization of the voter process, of which each particle is of type A or a. Under suitable scaling, the local proportion functions of A particles converge to continuous functions which solve a class of stochastic partial differential equations driven by Fisher-Wright white noise. To obtain the convergence, the tightness of these functions is derived from the moment estimate method.</p>
            </div>
        

        
            
            <div class="bibtex hidden">
            <div class="post-content highlight"><pre><code class="bibtex">@article&#123zhao2022limits,
            title   = "Limits of one-dimensional interacting particle systems with two-scale interaction",
            author  = "Tong Zhao",
            year    = "2022",<br/>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;journal = "Chinese Annals of Mathematics, Series B",<br/>&#125
            </code></pre></div>
            </div>
        

            
    </div>  
</div> 
</li></ol> 


</div>
</main>

<footer class="footer" style="clear: both;">
    <span>
        Copyright
        &copy;
        2023-2024
        <a href="https://zhaotong94.github.io/" style="color:#939393;">Tong&#39;Log</a>
        <span>
            Powered by
            <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
            <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
        </span>
    </span>
    
    <span id="busuanzi_container">
        <span class="fa fa-user"></span> <span id="busuanzi_value_site_uv"></span>
        <span class="fa fa-eye"></span> <span id="busuanzi_value_site_pv"></span>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50 && window.getSelection().anchorNode.parentNode.nodeName !== 'CODE') {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r\n' +
                    
                
                    "Copyright Notice: This article is an original piece by '+Tong\u0027Log+', licensed under the CC 4.0 BY-SA copyright agreement. Please include the original source link and this statement when reprinting." + "\r\n Original link: " + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\n‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\r\n' +
                    
                
                    "Copyright Notice: This article is an original piece by '+Tong\u0027Log+', licensed under the CC 4.0 BY-SA copyright agreement. Please include the original source link and this statement when reprinting." + "\r\n Original link: " + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = '‚úÇÔ∏ècopy';

        function copyingDone() {
            copybutton.innerText = 'üëåüèªcopied!';
            setTimeout(() => {
                copybutton.innerText = '‚úÇÔ∏ècopy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                let text = codeblock.textContent
                navigator.clipboard.writeText(text);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {}
            selection.removeRange(range);
        });

        let language = codeblock.className.replaceAll("language-", "")
        let macTool = document.createElement("div")
        let macTool1 = document.createElement("div")
        let macTool2 = document.createElement("div")
        let macTool3 = document.createElement("div")
        let languageType = document.createElement("div")
        languageType.innerText = language
        macTool.setAttribute('class', 'mac-tool')
        macTool1.setAttribute('class', 'mac bb1')
        macTool2.setAttribute('class', 'mac bb2')
        macTool3.setAttribute('class', 'mac bb3')
        languageType.setAttribute('class', 'language-type')
        macTool.appendChild(macTool1)
        macTool.appendChild(macTool2)
        macTool.appendChild(macTool3)
        macTool.appendChild(languageType)

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
            container.appendChild(macTool)
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
            container.appendChild(macTool)
        }
    });
</script>







<script>
    $(document).ready(function () {
        $("#generatePDF").click(function () {
            
            const content = document.getElementById("resume").cloneNode(true);
            content.style.display = "block"; 

            
            content.style.transform = "scale(0.75)"; 
            content.style.transformOrigin = "top center"; 
         
            
            html2pdf()
            .from(content)  
            .set({
                html2canvas: {
                    scale: 3,                
                    logging: true,           
                    useCORS: true,           
                    allowTaint: true,        
                    backgroundColor: "#F0F0F0", 
                    
                    
                },
                jsPDF: {
                    unit: "mm",              
                    format: "a4",            
                    orientation: "portrait", 
                    margins: {               
                        top: 20,             
                        left: 20,            
                        bottom: 20,          
                        right: 20            
                    }
                }
            })
            .save("tongzhao_cv.pdf");  
        });
    });
    </script>
    
    
    
    </body>

</html>
