[{"content":"I would like to apply for a postdoctoral position in the AI-related field. I have a multidisciplinary background in mathematics, AI, high-performance computing and following advantages:\nCuriosity, Adaptability, and Self-Motivation. I am capable of working independently and eager to learn new knowledge, even willing to adopt a doctoral student mindset. I aim to contribute to highly original research. Additionally, I am open to discussing ideas with group members and assisting in guiding PhD students. Computational Resources. Due to my professional experience in the High Performance Computer Research Center, I have established good collaborative relationships with some leading supercomputing companies, internet companies, as well as distributed training centers in universities and research institutes. If necessary, I can provide the required computational resources for our group. Relevant Theoretical Foundation. I obtained my Ph.D. in the School of Mathematical Sciences, with strong algorithm analysis skills and the ability to learn new theories. My background covers deep learning optimizers, reinforcement learning, stochastic analysis, control and game theory, and PDEs. In addition, almost all my personal information (referees, publications, and etc.) related to an postdoctoral application can be found on this website. If you have any interest and question, please feel free to contact me or refer to the FAQ section.\n","permalink":"https://zhaotong94.github.io/news/my-first-news/","summary":"I would like to apply for a postdoctoral position in the AI-related field. I have a multidisciplinary background in mathematics, AI, high-performance computing and following advantages:\nCuriosity, Adaptability, and Self-Motivation. I am capable of working independently and eager to learn new knowledge, even willing to adopt a doctoral student mindset. I aim to contribute to highly original research. Additionally, I am open to discussing ideas with group members and assisting in guiding PhD students. Computational Resources. Due to my professional experience in the High Performance Computer Research Center, I have established good collaborative relationships with some leading supercomputing companies, internet companies, as well as distributed training centers in universities and research institutes. If necessary, I can provide the required computational resources for our group. Relevant Theoretical Foundation. I obtained my Ph.D. in the School of Mathematical Sciences, with strong algorithm analysis skills and the ability to learn new theories.","title":"My First News"},{"content":"Motivation When giving a presentation on STEM disciplines, we usually need to draw a gif of some dynamical systems to express our ideas clearly. However, creating a portable, fluid, and authentic gif is challenging, it is very hard to find suitable parameters for drawing a gif.\nChallenge No matter how long this process takes, we only have a limited time to watch (usually very short). How to define fluidity of an animation (comfortable to watch)? Due to the stability of the discretize dynamical system of interest, we can not arbitrarily choose a stepsize for the system. What is the relationship between the size of the created gif and fluidity? Analysis We define three concept for $i\\in\\{r,o\\}$: $$\r\\begin{align}\rD_i:\u0026=\\frac{\\Delta_i}{F_i},\\label{d}\\\\\rR_i:\u0026=\\frac{F_i}{T_i},\\\\\rS_i:\u0026=\\frac{R_i}{D_i},\\\\\rv_i:\u0026=D_i R_i, \\label{v}\r\\end{align}\r$$ where $r$ and $o$ represent the above quantities is measured in reality and observation respectively, $\\Delta_i$ is the total variation, or the total path length of things moving in the image measured by some reasonable metrics, $F_i$ is total number of images, $T_i$ is total time, $D_i$ is resolution (the average difference between two adjacent images in the series), $R_i$ is frame rate, $S_i$ is fluidity or smoothness of a gif, and $v_i$ is the velocity of things in images.\nIn our creating-a-gif-for-dynamical-systems case, $R_r$ is how many images are calculated per unit real time. Considering $$\r\\begin{align*}\r\\dot{\\theta}_t\u0026=-L^{\\prime}(t,\\theta_t),\\\\\r\\theta_{t+1} \u0026=\\theta_t-\\eta L^{\\prime}(t,\\theta_t),\r\\end{align*}\r$$ where $\\eta$ is learning rate. At every time step $t$, an image $\\theta_{t}$ is generated, and therefore $R_r=\\frac{1}{\\eta}$.\nIn order to guarantee the stability of the dynamical system, we have to increase $R_r$. However, if we save all these easy-to-calculate images into gif it is usually lead to prohibitively large gif file. So, we need to sample some images with compression ratio $k=\\frac{F_r}{F_o}$ and store them into gif. To keep the gif authentic (without distortion), we set $l=\\frac{T_r}{T_o}$, acceleration ratio, a constant (otherwise the process distorted) and therefore $$\r\\begin{align*}\rk \\eta\u0026=\\frac{l}{R_o},\\\\\r\\end{align*} $$ where $R_o$ is the FPS of gif (a constant). According to equations \\eqref{d}-\\eqref{v}, we have $$\r\\begin{align*}\rS_o\u0026=\\frac{R_o}{D_o}=\\frac{F^2_o}{T_o\\Delta}=\\frac{F^2_o}{T_o T_r v_r},\\\\\rF_o\u0026=\\frac{R_r T_r}{k}, \\end{align*}\r$$ where $\\Delta=\\Delta_r=\\Delta_o$ in this case, $F_o$ is proportional to the size of the gif file. Finally, we get the suitable compression ratio $k$ and corresponding gif size measured by $F_o$ $$\r\\begin{align*}\rk\u0026=\\frac{\\sqrt{T_r}}{\\eta\\sqrt{S_o T_o v_r}}, \\; v_r=|L^{\\prime}|,\\\\\rF_o\u0026=\\sqrt{S_o T_o T_r v_r}.\r\\end{align*}\r$$ ","permalink":"https://zhaotong94.github.io/blog/hcssdgds/","summary":"Motivation When giving a presentation on STEM disciplines, we usually need to draw a gif of some dynamical systems to express our ideas clearly. However, creating a portable, fluid, and authentic gif is challenging, it is very hard to find suitable parameters for drawing a gif. Challenge No matter how long this process takes, we only have a limited time to watch (usually very short). How to define fluidity of an animation (comfortable to watch)? Due to the stability of the discretize dynamical system of interest, we can not arbitrarily choose a stepsize for the system. What is the relationship between the size of the created gif and fluidity? Analysis We define three concept for $i\\in\\{r,o\\}$: $$ \\begin{align} D_i:\u0026=\\frac{\\Delta_i}{F_i},\\label{d}\\\\ R_i:\u0026=\\frac{F_i}{T_i},\\\\ S_i:\u0026=\\frac{R_i}{D_i},\\\\ v_i:\u0026=D_i R_i, \\label{v} \\end{align} $$ where $r$ and $o$ represent the above quantities is measured in reality and observation","title":"How to create a small and smooth dynamic GIF for dynamical systems?"},{"content":"代码生成算法 pip install latexify-py import latexify\n表格生成 table\n图像生成 draw\n","permalink":"https://zhaotong94.github.io/tech/paper_machine/","summary":"代码生成算法 pip install latexify-py import latexify 表格生成 table 图像生成 draw","title":"Paper_machine"},{"content":"","permalink":"https://zhaotong94.github.io/tech/latex_cmd/","summary":"","title":"Latex_cmd"},{"content":"用户目录下载anaconda\ncurl anacoda 建立环境\nconda create --name \u0026lt;env_name\u0026gt; python=3.9 激活环境\nconda activate \u0026lt;env_name\u0026gt; 安装pytorch，官网寻找命令，注意与自己的cuda匹配\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 删除conda环境\nconda env remove --name \u0026lt;env_name\u0026gt; 查看所有模块\nmodule av 加载指定模块\nmodule load \u0026lt;module_name\u0026gt; 添加环境变量\nexport PATH=\u0026lt;path_to_dir\u0026gt;:$PATH 列出当前已加载模块\nmodule list 查询是哪个可执行二进制文件\nwhich xxx ","permalink":"https://zhaotong94.github.io/tech/build_ai_env/","summary":"用户目录下载anaconda curl anacoda 建立环境 conda create --name \u0026lt;env_name\u0026gt; python=3.9 激活环境 conda activate \u0026lt;env_name\u0026gt; 安装pytorch，官网寻找命令，注意与自己的cuda匹配 pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 删除conda环境 conda env remove --name \u0026lt;env_name\u0026gt; 查看所有模块 module av 加载指定模块 module load \u0026lt;module_name\u0026gt; 添加环境变量 export PATH=\u0026lt;path_to_dir\u0026gt;:$PATH 列出当前已加载模块 module list 查询是哪个可执行二进制文件 which xxx","title":"Build_AI_env"},{"content":"下载文件\nwget -O url_link 或者\ncurl -O url_link 使用代理\nproxychains4 wget -O url_link 或者\nproxychains4 curl -O url_link proxychains4 使用ssh从当前计算机传递当前文件夹下所有文件到远程计算机的指定目录\nscp -r ./* ./.??* user_name@IP_remote:~/path2traget_dir ","permalink":"https://zhaotong94.github.io/tech/remote_cmd/","summary":"下载文件 wget -O url_link 或者 curl -O url_link 使用代理 proxychains4 wget -O url_link 或者 proxychains4 curl -O url_link proxychains4 使用ssh从当前计算机传递当前文件夹下所有文件到远程计算机的指定目录 scp -r ./* ./.??* user_name@IP_remote:~/path2traget_dir","title":"Remote_cmd"},{"content":"打开文件：vim \u0026lt;file_name\u0026gt;\n编辑模式：i\n退出编辑模式：esc\n保存并退出：:x\n复制：非编辑模式下，光标起始位V，光标结束位y。 粘贴：非编辑模式下，光标位后p, 光标位前P。\n","permalink":"https://zhaotong94.github.io/tech/vim_cmd/","summary":"打开文件：vim \u0026lt;file_name\u0026gt; 编辑模式：i 退出编辑模式：esc 保存并退出：:x 复制：非编辑模式下，光标起始位V，光标结束位y。 粘贴：非编辑模式下，光标位后p, 光标位前P。","title":"Vim_cmd"},{"content":"提交任务\nsbatch \u0026lt;file_name.sh\u0026gt; 查看提交队列信息\nsqueue \u0026lt;file_name.sh\u0026gt; 查看详细提交队列信息\nsqueue -o \u0026#34;%.18i %.9P %.10j %.8u %.8T %.10M %.9l %.6D %N %.20b\u0026#34; 登录并独占某一节点，实行交互式作业\nsalloc -p your_partition_name -N1 ","permalink":"https://zhaotong94.github.io/tech/slurm_cmd/","summary":"提交任务 sbatch \u0026lt;file_name.sh\u0026gt; 查看提交队列信息 squeue \u0026lt;file_name.sh\u0026gt; 查看详细提交队列信息 squeue -o \u0026#34;%.18i %.9P %.10j %.8u %.8T %.10M %.9l %.6D %N %.20b\u0026#34; 登录并独占某一节点，实行交互式作业 salloc -p your_partition_name -N1","title":"Slurm_cmd"},{"content":"新建本地仓库\ngit init 添加到暂存区\ngit add . 添加到仓储区\ngit commit -m \u0026#34;content for labeling this submission\u0026#34; 删除\ngit remove 删除仓库\ngit remove 删除分支\ngit remove 新建分支\ngit branch 查看分支\ngit branch 切换分支\ngit switch 重新命名分支\ngit 连接远程（github）分支\ngit remote add 创建ssh密钥\nssh key-gen ","permalink":"https://zhaotong94.github.io/tech/git_cmd/","summary":"新建本地仓库 git init 添加到暂存区 git add . 添加到仓储区 git commit -m \u0026#34;content for labeling this submission\u0026#34; 删除 git remove 删除仓库 git remove 删除分支 git remove 新建分支 git branch 查看分支 git branch 切换分支 git switch 重新命名分支 git 连接远程（github）分支 git remote add 创建ssh密钥 ssh key-gen","title":"Git_cmd"},{"content":"\u0026lt;div\u0026gt; 科技代码 科技代码 科技代码 \u0026lt;/div\u0026gt; 1. 介绍 scan命令的作用和keys *的作用类似，主要用于查找redis中的键，但是在正式的生产环境中一般不会直接使用keys *这个命令，因为他会返回所有的键，如果键的数量很多会导致查询时间很长，进而导致服务器阻塞，所以需要scan来进行更细致的查找\nscan总共有这几种命令：scan、sscan、hscan、zscan，分别用于迭代数据库中的：数据库中所有键、集合键、哈希键、有序集合键，命令具体结构如下：\nscan cursor [MATCH pattern] [COUNT count] [TYPE type] sscan key cursor [MATCH pattern] [COUNT count] hscan key cursor [MATCH pattern] [COUNT count] zscan key cursor [MATCH pattern] [COUNT count] 2. scan scan cursor [MATCH pattern] [COUNT count] [TYPE type]，cursor表示游标，指查询开始的位置，count默认为10，查询完后会返回下一个开始的游标，当返回0的时候表示所有键查询完了\n127.0.0.1:6379[2]\u0026gt; scan 0 1) \u0026#34;3\u0026#34; 2) 1) \u0026#34;mystring\u0026#34; 2) \u0026#34;myzadd\u0026#34; 3) \u0026#34;myhset\u0026#34; 4) \u0026#34;mylist\u0026#34; 5) \u0026#34;myset2\u0026#34; 6) \u0026#34;myset1\u0026#34; 7) \u0026#34;mystring1\u0026#34; 8) \u0026#34;mystring3\u0026#34; 9) \u0026#34;mystring4\u0026#34; 10) \u0026#34;myset\u0026#34; 127.0.0.1:6379[2]\u0026gt; scan 3 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;myzadd1\u0026#34; 2) \u0026#34;mystring2\u0026#34; 3) \u0026#34;mylist2\u0026#34; 4) \u0026#34;myhset1\u0026#34; 5) \u0026#34;mylist1\u0026#34; $$\r\\begin{enumerate}\r\\item asfd\r\\item adf\r\\end{enumerate}\r$$ ","permalink":"https://zhaotong94.github.io/tech/tech1/","summary":"\u0026lt;div\u0026gt; 科技代码 科技代码 科技代码 \u0026lt;/div\u0026gt; 1. 介绍 scan命令的作用和keys *的作用类似，主要用于查找redis中的键，但是在正式的生产环境中一般不会直接使用keys *这个命令，因为他会返回所有的键，如果键的数量很多会导致查询时间很长，进而导致服务器阻塞，所以需要scan来进行更细致的查找 sc","title":"Redis scan"},{"content":"Motivation The direct motivation of RKHS is to extend the following concepts and properties in $\\mathbb{R}^n$\nthe concept of evaluation functional $L_i$ at point $i\\in[n]$ in $\\mathbb{R}^n$ $$\rL_{i}:x\\mapsto \\left\\langle \\delta_i, x\\right\\rangle=x_i, \\forall x\\in\\mathbb{R}^n,\r$$ where $\\left\\langle \\cdot, \\cdot\\right\\rangle$ is linear functional notation and $\\delta_i(\\cdot)$ is Dirac function with mass at $i$, the eigenvector $e_i:=(0,0,\\cdots,1,\\cdots,0)$ in $\\mathbb{R}^n$ such that $$\r\\left\\langle \\delta_i, x\\right\\rangle=\\left\\langle e_i, x\\right\\rangle_{\\mathbb{R}^n}, \\forall x\\in\\mathbb{R}^n,\r$$ and more generally there exists $k_i$ in Hibert space with inner product $\\left\\langle \\cdot, \\cdot\\right\\rangle_H$ such that $$\r\\left\\langle \\delta_i, x\\right\\rangle=\\left\\langle k_i, x\\right\\rangle_{H}, \\forall x\\in\\mathbb{R}^n,\r$$ the property of expressing inner product $\\left\\langle x, y\\right\\rangle_H$ in $\\mathbb{R}^n$ as a production $$\rx^{\\textsf{T}}Hy, \\forall x,y\\in\\mathbb{R}^n,\r$$ the property that there exists an orthonormal basis $\\{e_1,e_2,\\cdots,e_n\\}$ bases in $ \\mathbb{R}^n $ such that the above positive definite matrix can be denoted as a diagonal matrix $$\rU=(e_1,e_2,\\cdots,e_n),\rU^{\\textsf{T}}HU=\\text{diag}(\\lambda_1,\\lambda_2,\\cdots,\\lambda_n)=\\sum_{i=1}^n\\lambda_ie_ie_i^\\textsf{T},\r$$ to corresponding ones in infinite dimensional linear space $\\mathcal{B}$ like followings, and we hope: an evalutation functional $L_x$ at point $x\\in\\mathcal{X}$ in infinite dimensional linear space $\\mathcal{B}(\\mathcal{X})$ of functions on set $\\mathcal{X}$ (usually card$(\\mathcal{X})\\geq\\aleph_0$) can be defined as $$\rL_{x}:f\\mapsto \\left\\langle \\delta_x, f\\right\\rangle=f(x), \\forall x\\in\\mathcal{X}, f\\in\\mathcal{B},\r$$ the eigenvector $\\delta_x(\\cdot)$ in $\\mathbb{L}^2(\\mathcal{X},\\mu)$ such that $$\r\\left\\langle \\delta_x, f\\right\\rangle=\\left\\langle \\delta_x, f\\right\\rangle_{\\mathbb{L}^2}, \\forall f\\in\\mathbb{L}^2(\\mathcal{X},\\mu),\r$$ where $\\mu$ is a measure, and more generally there exists $K_x(\\cdot)\\in\\mathcal{H}$ can be defined as $$\r\\left\\langle \\delta_x, f\\right\\rangle=\\left\\langle K_x, f\\right\\rangle_{\\mathcal{H}}, \\forall f\\in\\mathcal{H},\r$$ the inner product in Hilbert space $\\mathcal{H}$ can be defined as a binary integral form $$\r\\left\\langle f, g\\right\\rangle_\\mathcal{H}=\\int_{\\mathcal{X}\\times\\mathcal{X}} f(x)h(x,y)g(y)dxdy, \\forall x,y \\in\\mathcal{X}, f,g \\in\\mathcal{H},\r$$ there exists a set of orthonormal basis $\\phi(\\cdot,z)$ such that the inner product in a Hilbert space can be expressed in a \u0026ldquo;diagonal matrix\u0026rdquo; form, $$\r\\begin{align*}\r\\phi(z,w)^{\\textsf{T}}\u0026=\\phi(w,z),\\int_{\\mathcal{X}\\times\\mathcal{X}} \\phi^{\\textsf{T}}(z,w)h(w,z)\\phi(z,y)dwdz\\\\\r\u0026=\\int_{\\mathcal{X}\\times\\mathcal{X}} \\sigma(z)\\phi(x,z)\\phi(y,z)dz\\\\\r\u0026=\\sigma(y)\\delta(x-y).\r\\end{align*}\r$$ Conditions, Problems, and Solutions It is obviously that $L_x$ exists. Due to the motivation involves inner product, $\\mathcal{B}$ must be a Hilbert space $\\mathcal{B}:=\\mathcal{H}$.\nTo express the evaluation functional $L_x$ in the form of an inner product, we need to require that the linear functional is continuous (i.e. $sup_{f\\in\\mathcal{H}}\\frac{|L_x(f)|}{\\|f\\|_\\mathcal{H}}\u003c\\infty, \\forall x\\in\\mathcal{X}$), since the inner product must be continuous (i.e.$\\left\\langle f, g\\right\\rangle_\\mathcal{H}\\leq{\\|f\\|_\\mathcal{H}}{\\|g\\|_\\mathcal{H}}$). Then, the Riesz representation theorem implies that there exists a unique elements $K_x\\in\\mathcal{H}$ called reproducing kernel such that $\\left\\langle K_x, f\\right\\rangle_{\\mathcal{H}}=f(x),\\forall f\\in \\mathcal{H}$. Therefore, $\\left\\langle K_x, K_y\\right\\rangle_{\\mathcal{H}}=K_y(x):=K(y,x)$. Due to the sysmetry of inner product, $K(x,y)=K(y,x)$. In addition, $$\r\\mathcal{H}_0:=\\{f|f\\in\\overline{\\text{span}}\\{K_x(\\cdot)|x\\in{\\mathcal{X}}\\},\\left\\langle f, f\\right\\rangle_{\\mathcal{H}}\u003c\\infty\\}=\\mathcal{H}.\r$$ Considering Due to the motivation involves integral on $\\mathcal{X}$, we must introduce non-negative measure $\\mu$ (the sign is put into coefficient $f_K$) and define $$f:=\\int f_K(x)K(x,\\cdot)\\mu(dx)=\\sum_{i=1}^{\\infty}f_K(i)K(x_i,\\cdot)\u003c\\infty.$$ where $\\{K(x,\\cdot)|x\\in\\mathcal{X}, s.t. K(x,\\cdot)\\text{s} \\; \\text{form a basis with}\\;x\\;\\text{as less as possible}\\}=E$ is a basis set, since Hilbert space is complete (any point in it can be approximated by finite linear combination of basis). If $\\mathcal{H}$ is separable (e.g. $\\mathbb{L}^2(\\mathbb{R})$ and $\\mathbb{L}^2([0,1])$ with orthonormal basis Hermite polynomials $H_n(x)$ times $e^{-x^2/2}$ and Legendre polynomials), $\\text{card}(E)\\leq\\aleph_0$. If $\\text{card}(E)\u003e\\aleph_0$, we can choose a countable subset $E_0\\subset E$ to define $f$.\nOf course, we have $$\r\\begin{align*}\r\\|f\\|^2_\\mathcal{H}\u0026=\\left\\langle\\int f_K(x)K(x,\\cdot)\\mu(dx),\\int f_K(y)K(y,\\cdot)\\mu(dy)\\right\\rangle_{\\mathcal{H}}\\\\\r\u0026=\\int f_K(x)K(x,y)f_K(y)\\mu(dx)\\mu(dy)\\\\\r\u0026=\\int f(x)f_K(x)\\mu(dx)\u003c\\infty.\r\\end{align*}\r$$ The inner product is $$\r\\begin{align*}\r\\left\\langle f, g\\right\\rangle_\\mathcal{H}\u0026=\\left\\langle\\int f_K(x)K(x,\\cdot)\\mu(dx),\\int g_K(y)K(y,\\cdot)\\mu(dy)\\right\\rangle_{\\mathcal{H}}\\\\\r\u0026=\\int f_K(x)K(x,y)g_K(y)\\mu(dx)\\mu(dy)\u003c\\infty.\r\\end{align*}\r$$ For any generaly linearly independent basis set $E$ of $\\mathcal{H}$, and measure $\\mu_E$ on $E$, it behaves like in finite dimensional case $$\r\\left\\langle f, g\\right\\rangle_\\mathcal{H}=\\int_E f_E(e_x)H_E(e_x,e_y)g_E(e_y)\\mu_E(de_x)\\mu_E(de_y),\r$$ where $H_E$ is the representation of $\\left\\langle \\cdot, \\cdot \\right\\rangle_\\mathcal{H}$ and $f_E$ is the representation (coefficient) of $f$ under $E$ ($H_E(e_x,e_y):=\\left\\langle e_x, e_y\\right\\rangle_\\mathcal{H}, e_x(\\cdot), e_y(\\cdot)\\in E, f=\\int_E f_E(e_x)e_x(\\cdot)\\mu_E(de_x)$). If we can identify $(E,\\mu_E)$ with $(\\mathcal{x},\\mu) $, we have $$\r\\left\\langle f, g\\right\\rangle_\\mathcal{H}=\\int_\\mathcal{X} f_E(x)H_E(x,y)g_E(y)\\mu(dx)\\mu(dy).\r$$ For general cases, the measure $\\mu_E$ or $\\mu$ is count measure. Using Zorn\u0026rsquo;s Lemma, we know there exist a orthonormal basis $E$ of $\\mathcal{H}$, under which $$\r\\left\\langle f, g\\right\\rangle_\\mathcal{H}=\\int_E f_E(e_x)g_E(e_x)\\mu_E(de_x).\r$$ Fortunately, card($\\{K_x(\\cdot)\\})=\\aleph_0$ usually holds, but what is the condition required. Of course, $\\mathcal{H}$ is separable is a sufficient condition, but not pragmatic. In fact, if we regard it as a non-negative definite matrix $K(x,y)$ we hope we can obtain \\begin{equation*} K(x,y)=\\sum_{i=1}^{\\infty}\\sigma_i\\phi_i(x)\\phi_i(y). \\end{equation*} Luckily, the spectral theory guarantees the above equation holds, if $K$ is a compact operator. According to the requirement of spectral theory, one of following conditions should be satisfied:\nwe let $\\mu$ is $\\sigma$-finite measure, $K$ is squared-integrable on $\\mu\\otimes\\mu$, and then we find $T_K:\\mathbb{L}^{2}(\\mathcal{X})\\to \\mathbb{L}^{2}(\\mathcal{X})$, $$[T_{K}f](\\cdot)=\\int _{\\mathcal{X}}K(\\cdot,x)f(x)\\,d\\mu (x)$$ is a Hilbert–Schmidt operator (compact operator). Mercer\u0026rsquo;s theorem. we let $K$ is continuous, $\\mathcal{X}$ is compact, $\\mu$ is positive finite Borel measure, and then we get $T_K:\\mathbb{L}^{2}(\\mathcal{X})\\to \\mathbb{L}^{2}(\\mathcal{X})$ as $$[T_{K}f](\\cdot)=\\int _{\\mathcal{X}}K(\\cdot,x)f(x)\\,d\\mu (x)$$ is compact operator. In both cases, $K(x, \\cdot)\\in \\mathbb{L}^{2}(\\mathcal{X},\\mu),\\;\\forall x\\in\\mathcal{X}$. $K(x,y)$ can be regard as inner product \u0026lsquo;martix\u0026rsquo; in both $\\mathbb{L}^{2}(\\mathcal{X},\\mu)$ space $(\\left\\langle e_x, e_y\\right\\rangle_{\\mathbb{L}^{2}})$ and $\\mathcal{H}$ space $(\\left\\langle K_x, K_y\\right\\rangle_\\mathcal{H})$. From $\\mathbb{L}^{2}(\\mathcal{X},\\mu)$ space perspective, $$\r\\begin{align}\rK(x,y)\u0026=\\sum_{i=1}^{\\infty}\\sigma_i\\phi_i(x)\\phi_i(y).\\label{k}\r\\end{align}\r$$ From $\\mathcal{H}$ space perspective, under basis $$\rh_i(\\cdot)=\\int K(\\cdot,x)\\phi_i(x)\\mu(dx)=\\sigma_i\\phi_i(\\cdot),\r$$ inner product $\\left\\langle \\cdot, \\cdot\\right\\rangle_\\mathcal{H}$ can be written as diag($\\sigma_1,\\sigma_2,\\cdots$), hence $$\r\\left\\langle h_i, h_j\\right\\rangle_\\mathcal{H}=\\sigma_i\\sigma_j\\left\\langle \\phi_i, \\phi_j\\right\\rangle_\\mathcal{H}=\\sigma_i\\delta_{ij}=\\sigma_i\\left\\langle \\phi_i, \\phi_j\\right\\rangle_{\\mathbb{L}^{2}(\\mathcal{X})} ,\r$$ For case one, we have $\\sum_i \\sigma_i^2\u003c\\infty$ ($\\sigma_i\\to 0$), therefore Im$(T_{K})\\subset\\mathcal{H}\\subset\\mathbb{L}^{2}(\\mathcal{X})$. Conditions in case two is strictly stronger than that in one, it comes to the same conclusion as in case one. The difference is the decomposition of $K$ \\eqref{k} in case one converges in $\\mathbb{L}^{2}$ norm rather than uniformly in case two.\nRemark: Conditions on reproducing kernel $K(\\cdot, \\cdot)$ (integrable, continuous) embed $\\mathcal{H}$ into a countable dimensional subspace of $\\mathbb{R}^\\mathcal{X}$ without considering the topology on these two space. For any $h\\in\\mathcal{H}$, we can not evaluate $h(x)$ at $x$ randowly, as values at different points fulfill some constraints inherted from reproducing kernel $K$. The operator $T_K$ can be seen as a infinite dimensional version SVD decomposition. Due to bounded $\\sigma_i$, $T_K$ compresses and projects a point in $\\mathbb{L}^2$ to $\\mathcal{H}$. Therefore, $$\r\\left\\langle f, g\\right\\rangle_\\mathcal{H}=\\left\\langle \\sum_i\\left\\langle f, \\phi_i\\right\\rangle_{\\mathbb{L}^{2}}\\phi_i , \\sum_j\\left\\langle f, \\phi_j\\right\\rangle_{\\mathbb{L}^{2}}\\phi_j\\right\\rangle_\\mathcal{H}=\\sum_i\\frac{\\left\\langle f, \\phi_i\\right\\rangle_{\\mathbb{L}^{2}}\\left\\langle g, \\phi_i\\right\\rangle_{\\mathbb{L}^{2}}}{\\sigma_i}.\r$$ Besides, considering an embedding map $I:\\mathcal{H}\\to\\mathbb{L}^{2}$, we find the map is continuous (i.e.$\\|x-y\\|_{\\mathbb{L}^{2}}\\leq \\|I(x)-I(y)\\|_\\mathcal{H},\\; \\forall x,y\\in \\mathcal{H}$). Hence topologies ($\\mathcal{T}_{\\mathbb{L}^{2}}, \\mathcal{T}_\\mathcal{H}$) determinded by their metric have relationship $\\mathcal{T}_{\\mathbb{L}^{2}}\\subset\\mathcal{T}_\\mathcal{H}$.\nAll the conclusion can be applied to complex cases. Some values need to be written as their conjugates.\nExamples Gaussian linear polynomial ","permalink":"https://zhaotong94.github.io/blog/rkhs/","summary":"Motivation The direct motivation of RKHS is to extend the following concepts and properties in $\\mathbb{R}^n$ the concept of evaluation functional $L_i$ at point $i\\in[n]$ in $\\mathbb{R}^n$ $$ L_{i}:x\\mapsto \\left\\langle \\delta_i, x\\right\\rangle=x_i, \\forall x\\in\\mathbb{R}^n, $$ where $\\left\\langle \\cdot, \\cdot\\right\\rangle$ is linear functional notation and $\\delta_i(\\cdot)$ is Dirac function with mass at $i$, the eigenvector $e_i:=(0,0,\\cdots,1,\\cdots,0)$ in $\\mathbb{R}^n$ such that $$ \\left\\langle \\delta_i, x\\right\\rangle=\\left\\langle e_i, x\\right\\rangle_{\\mathbb{R}^n}, \\forall x\\in\\mathbb{R}^n, $$ and more generally there exists $k_i$ in Hibert space with inner product $\\left\\langle \\cdot, \\cdot\\right\\rangle_H$ such that $$ \\left\\langle \\delta_i, x\\right\\rangle=\\left\\langle k_i, x\\right\\rangle_{H}, \\forall x\\in\\mathbb{R}^n, $$ the property of expressing inner product $\\left\\langle x, y\\right\\rangle_H$ in $\\mathbb{R}^n$ as a production $$ x^{\\textsf{T}}Hy, \\forall x,y\\in\\mathbb{R}^n, $$ the property that there exists an orthonormal basis $\\{e_1,e_2,\\cdots,e_n\\}$ bases in $ \\mathbb{R}^n $ such that the above positive definite matrix can be denoted as a diagonal","title":"Reproducing Kernel Hilbert Space"},{"content":" 首页加coming，最近更新, 语录，添加文末讨论部分。 添加code，paper，supp，ppt，vedio，doi，abstract，blog，poster等模块。 hugo markdown作图 外微分，联络，黎曼几何 强化学习，控制 最优控制角度理解ddpm bsde的理解 马里万导数的理解 布尔巴基学派 范畴与函子 拟重力优化器 因式分解 \u0026ldquo;\\Climb heights never reached, gain perspectives nerve realized, discover things never seen, and create worlds never expected. \\ \u0026ndash;Tong \\ Reading broadens perspectives and inspires thought; writing perceives nuance and clarifies logic.\\ \u0026ndash;Tong\u0026rdquo; ","permalink":"https://zhaotong94.github.io/tech/memo/","summary":"首页加coming，最近更新, 语录，添加文末讨论部分。 添加code，paper，supp，ppt，vedio，doi，abstract，blog，poster等模块。 hugo markdown作图 外微分，联络，黎曼几何 强化学习，控制 最优控制角度理解ddpm bsde的理解 马里万导数的理解 布","title":"Memo"},{"content":"Do you often encounter a shortage of letters when writing papers? The arbitrary use of letters earlier on can lead to conflicts when defining new variables later, and changing already defined letters can result in inevitable tedious modifications.\nHave you ever read papers where undefined letters appear and often affect understanding? It often requires repeatedly searching and wasting a lot of time, and more importantly, you may not necessarily find them.\nTo solve the above two problems, I have compiled a list of common letter usages in the field of computer science and mathematics (case insensitive), and this table will be updated periodically.\nA: action\nB: borel base,batch\nC: constant, channel\nD: data, degree, distance (metric), dimension\nE: edge, element, expectation\nF: filter, function, face\nG: graph, function\nH: Hilbert, function, height, Hessian\nI: dumb index, identifier, imaginary_unit\nJ: dumb index\nK: key, dumb index, constant\nL: length, operator\nM: matrix, measure\nN: positive_integer, total_number\nO: order\nP: permutation，probability\nQ: query, rational_numer，probability\nR: real, reward\nS: sample, set, state\nT: time, topology, transform, temperature\nU: variable, open set, domain\nV: variable, open set, range, vertex\nW: variable, width, weight\nX: input, unknown_number, variate\nY: output, unknown_number, variate\nZ: unknown_number, variate, integer\n$\\alpha$: constant\n$\\beta$: coefficient for EMA, costant\n$\\chi$: Chi-squared distribution, statistics, constant,\n$\\delta$: delta function, $\\epsilon$-$\\delta$ language\n$\\epsilon$: $\\epsilon$-$\\delta$ language\n$\\varepsilon$: $\\epsilon$-$\\delta$ language\n$\\phi$: function\n$\\gamma$: function, constant\n$\\eta$: learning_rate $\\iota$: including_map\n$\\kappa$: constant, $\\kappa$-coefficient\n$\\lambda$: eigenvalue, laplacian_factor\n$\\mu$: measure, distribution, mean\n$\\nu$: photon, constant\n$\\omicron$:infinitesimal\n$\\pi$: policy, distribution, measure\n$\\theta$: parameter\n$\\rho$: density\n$\\sigma$: sum, standard_deviation, variation\n$\\tau$: permutation, stopping time\n$\\upsilon$: displacement, constant\n$\\omega$: order, domain, open set\n$\\xi$: variate\n$\\psi$: function\n$\\zeta$: variate\nFor the first issue, when writing a paper and needing to define a new letter but without a particularly good choice, select letters whose common usage （listed above） do definitely not appear your paper for use.\nFor the second issue, when encountering undefined letters, directly check the above list to see if some common usage has a reasonable explanation within the paper.\nFor any republication, please contact me.\n","permalink":"https://zhaotong94.github.io/tech/tech/","summary":"Do you often encounter a shortage of letters when writing papers? The arbitrary use of letters earlier on can lead to conflicts when defining new variables later, and changing already defined letters can result in inevitable tedious modifications. Have you ever read papers where undefined letters appear and often affect understanding? It often requires repeatedly searching and wasting a lot of time, and more importantly, you may not necessarily find them. To solve the above two problems, I have compiled a list of common letter usages in the field of computer science and mathematics (case insensitive), and this table will be updated periodically. A: action B: borel base,batch C: constant, channel D: data, degree, distance (metric), dimension E: edge, element, expectation F: filter, function, face G: graph, function H: Hilbert, function, height, Hessian I: dumb index, identifier, imaginary_unit J: dumb","title":"Running out of letters? Unclear meanings in papers?"},{"content":"","permalink":"https://zhaotong94.github.io/cv/","summary":"","title":"👨‍🦱CV"},{"content":"@inproceedings{hu2023rlekf, abbr={AAAI}, title={RLEKF: An optimizer for deep potential with ab initio accuracy}, author={Hu*, Siyu and Zhang*, Wentao and Sha, Qiuchen and Pan, Feng and Wang, Lin-Wang and Jia, Weile and Tan, Guangming and Zhao†, Tong}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, volume={37}, number={7}, pages={7910–7918}, year={2023}, abstract={It is imperative to accelerate the training of neural network force field such as Deep Potential, which usually requires thousands of images based on first-principles calculation and a couple of days to generate an accurate potential energy surface. To this end, we propose a novel optimizer named reorganized layer extended Kalman filtering (RLEKF), an optimized version of global extended Kalman filtering (GEKF) with a strategy of splitting big and gathering small layers to overcome the $O(N^2)$ computational cost of GEKF. This strategy provides an approximation of the dense weights error covariance matrix with a sparse diagonal block matrix for GEKF. We implement both RLEKF and the baseline Adam in our αDynamics package and numerical experiments are performed on 13 unbiased datasets. Overall, RLEKF converges faster with slightly better accuracy. For example, a test on a typical system, bulk copper, shows that RLEKF converges faster by both the number of training epochs (×11.67) and wall-clock time (×1.19). Besides, we theoretically prove that the updates of weights converge and thus are against the gradient exploding problem. Experimental results verify that RLEKF is not sensitive to the initialization of weights. The RLEKF sheds light on other AI-for-science applications where training a large neural network (with tons of thousands parameters) is a bottleneck.}, bibtex_show={true}, pdf={https://arxiv.org/pdf/2212.06989}, code={}, blog={}, poster={}, slides={}, video={}, url={}, grade={oral}, doi={https://doi.org/10.1609/aaai.v37i7.25957}, altmetric={248277}, dimensions={true}, google_scholar_id={}, selected={true}, publisher={}, }\n@inproceedings{feng2024accelerating, abbr={EuroPar}, bibtex_show={true}, title={Accelerating large-scale sparse LU factorization for RF circuit simulation}, abstract={Sparse LU factorization is the indispensable building block of the circuit simulation, and dominates the simulation time, especially when dealing with large-scale circuits. Radio frequency (RF) circuits have been increasingly emphasized with the evolution of ubiquitous wireless communication (i.e., 5G and WiFi). The RF simulation matrices show a distinctive pattern of structured dense blocks, and this pattern has been inadvertently overlooked by prior works, leading to the underutilization of computational resources. In this paper, by exploiting the block structure, we propose a novel blocked format for L and U factors and re-design the large-scale sparse LU factorization accordingly, which leverages the data locality inherent in RF matrices. The data format transformation is streamlined, strategically eliminating the redundant data movement and costly indirect memory access. Moreover, the vector operations are converted into matrix operations, enabling efficient data reuse and enhancing data-level parallelism. The experiment results demonstrate that our method achieves superior performance compared to state-of-the-art implementation.}, author={Feng, Guofeng and Wang, Hongyu and Guo, Zhuoqiang and Li†, Mingzhen and Zhao, Tong and Jin, Zhou and Jia, Weile and Tan, Guangming and Sun, Ninghui}, booktitle={European Conference on Parallel Processing}, volume={}, number={}, pages={182–195}, year={2024}, publisher={Springer}, doi={https://doi.org/10.1007/978-3-031-69583-4_13}, }\n@inproceedings{hu2024training, abbr={PPoPP}, bibtex_show={true}, title={Training one deepmd model in minutes: A step towards online learning}, abstract={Neural Network Molecular Dynamics (NNMD) has become a major approach in material simulations, which can speedup the molecular dynamics (MD) simulation for thousands of times, while maintaining ab initio accuracy, thus has a potential to fundamentally change the paradigm of material simulations. However, there are two time-consuming bottlenecks of the NNMD developments. One is the data access of ab initio calculation results. The other, which is the focus of the current work, is reducing the training time of NNMD model. The training of NNMD model is different from most other neural network training because the atomic force (which is related to the gradient of the network) is an important physical property to be fit. Tests show the traditional stochastic gradient methods, like the Adam algorithms, cannot efficiently deploy the multisample minibatch algorithm. As a result, a typical training (taking the Deep Potential Molecular Dynamics (DeePMD) as an example) can take many hours. In this work, we designed a heuristic minibatch quasi-Newtonian optimizer based on Extended Kalman Filter method. An early reduction of gradient and error is adopted to reduce memory footprint and communication. The memory footprint, communication and settings of hyper-parameters of this new method are analyzed in detail. Computational innovations such as customized kernels of the symmetry-preserving descriptor are applied to exploit the computing power of the heterogeneous architecture. Experiments are performed on 8 different datasets representing different real case situations, and numerical results show that our new method has an average speedup of 32.2 compared to the Reorganized Layer-wised Extended Kalman Filter with 1 GPU, reducing the absolute training time of one DeePMD model from hours to several minutes, making it one step toward online training.}, author={Hu, Siyu and Zhao, Tong and Sha, Qiuchen and Li, Enji and Meng, Xiangyu and Liu, Lijun and Wang, Lin-Wang and Tan, Guangming and Jia, Weile}, booktitle={Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming}, pdf={https://dl.acm.org/doi/pdf/10.1145/3627535.3638505}, selected={true}, volume={}, number={}, pages={257–269}, year={2024}, publisher={}, }\n@inproceedings{li1950enhance, abbr={SC}, bibtex_show={true}, title={Enhance the strong scaling of lammps on fugaku}, abstract={Physical phenomenon such as protein folding requires simulation up to microseconds of physical time, which directly corresponds to the strong scaling of molecular dynamics(MD) on modern supercomputers. In this paper, we present a highly scalable implementation of the state-of-the-art MD code LAMMPS on Fugaku by exploiting the 6D mesh/torus topology of the TofuD network. Based on our detailed analysis of the MD communication pattern, we first adapt coarse-grained peer-to-peer ghost-region communication with uTofu interface, then further improve the scalability via fine-grained thread pool. Finally, Remote direct memory access (RDMA) primitives are utilized to avoid buffer overhead. Numerical results show that our optimized code can reduce 77% of the communication time, improving the performance of baseline LAMMPS by a factor of 2.9x and 2.2x for Lennard-Jones and embedded-atom method potentials when scaling to 36, 846 computing nodes. Our optimization techniques can also benefit other applications with stencil or domain decomposition methods.}, author={Li, Jianxiong and Zhao, Tong and Guo, Zuoqiang and Shi, Shunchen and Liu, Lijun and Tan, Guangming and Jia†, Weile and Yuan, Guojun and Wang†, Zhan}, booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, pdf={https://dl.acm.org/doi/pdf/10.1145/3581784.3607064}, volume={}, number={}, pages={1-13}, year={2023}, publisher={Association for Computing Machinery}, }\n@article{yan190510-million, bibtex_show={true}, abbr={JCST}, title={10-million atoms simulation of first-principle package LS3DF}, author={Yan, Yujin and Li†, HaiBo and Zhao, Tong and Wang, Lin-Wang and Shi, Lin and Liu, Tao and Tan, GuangMing and Jia†, Weile and Sun†, Ninghui}, abstract={The growing demand for semiconductor devices simulation poses a big challenge for large-scale electronic structure calculations. Among various methods, the linearly scaling three-dimensional fragment (LS3DF) method exhibits excellent scalability in large-scale simulations. Based on algorithmic and system-level optimizations, we propose a highly scalable and highly efficient implementation of LS3DF on a domestic heterogeneous supercomputer equipped with accelerators. In terms of algorithmic optimizations, the original all-band conjugate gradient algorithm is refined to achieve faster convergence, and mixed precision computing is adopted to increase overall efficiency. In terms of system-level optimizations, the original two-layer parallel structure is replaced by a coarse-grained parallel method. Optimization strategies such as multi-stream, kernel fusion, and redundant computation removal are proposed to increase further utilization of the computational power provided by the heterogeneous machines. As a result, our optimized LS3DF can scale to a 10-million silicon atoms system, attaining a peak performance of 34.8 PFLOPS (21.2% of the peak). All the improvements can be adapted to the next-generation supercomputers for larger simulations.}, journal={Journal of Computer Science and Technology}, volume={39}, number={1}, pages={45–62}, year={2024}, doi={https://doi.org/10.1007/s11390-023-3011-6}, award={}, award_name={}, }\n@article{zhao2022limits, bibtex_show={true}, abbr={CAM}, title={Limits of one-dimensional interacting particle systems with two-scale interaction}, author={Zhao, Tong}, abstract={This paper characterizes the limits of a large system of interacting particles distributed on the real line. The interaction occurring among neighbors involves two kinds of independent actions with different rates. This system is a generalization of the voter process, of which each particle is of type A or a. Under suitable scaling, the local proportion functions of A particles converge to continuous functions which solve a class of stochastic partial differential equations driven by Fisher-Wright white noise. To obtain the convergence, the tightness of these functions is derived from the moment estimate method.}, journal={Chinese Annals of Mathematics, Series B}, volume={43}, number={2}, pages={195-208}, year={2022}, doi={https://doi.org/10.1007/s11401-022-0311-z}, selected={true}, award={}, award_name={}, }\n","permalink":"https://zhaotong94.github.io/publications/","summary":"@inproceedings{hu2023rlekf, abbr={AAAI}, title={RLEKF: An optimizer for deep potential with ab initio accuracy}, author={Hu*, Siyu and Zhang*, Wentao and Sha, Qiuchen and Pan, Feng and Wang, Lin-Wang and Jia, Weile and Tan, Guangming and Zhao†, Tong}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, volume={37}, number={7}, pages={7910–7918}, year={2023}, abstract={It is imperative to accelerate the training of neural network force field such as Deep Potential, which usually requires thousands of images based on first-principles calculation and a couple of days to generate an accurate potential energy surface. To this end, we propose a novel optimizer named reorganized layer extended Kalman filtering (RLEKF), an optimized version of global extended Kalman filtering (GEKF) with a strategy of splitting big and gathering small","title":"📄Publications"},{"content":"Q: Why have you decided not to pursue a position as an assistant professor?/ A: There are three main reasons: first, I wish to engage in more creative research at institutions with higher research standards to satisfy my intellectual curiosity; second, I want to enrich my research and life experiences; and third, I am more passionate about artificial intelligence.// Q: What if I see something incorrect in the post?/ A: Please feel free to let me know，and I will correct it.// Q: Can I repost or translate the blog?/ A: It is my pleasure, but please clearly indicate that it is a repost and pin this notice at the top.\n","permalink":"https://zhaotong94.github.io/faq/","summary":"Q: Why have you decided not to pursue a position as an assistant professor?/ A: There are three main reasons: first, I wish to engage in more creative research at institutions with higher research standards to satisfy my intellectual curiosity; second, I want to enrich my research and life experiences; and third, I am more passionate about artificial intelligence.// Q: What if I see something incorrect in the post?/ A: Please feel free to let me know，and I will correct it.// Q: Can I repost or translate the blog?/ A: It is my pleasure, but please clearly indicate that it is a repost and pin this notice at the top.","title":"🤔FAQ"}]