[{"content":"I would like to apply for a postdoctoral position in the AI-related field. I have a multidisciplinary background in mathematics, AI, high-performance computing and following advantages:\nCuriosity, Adaptability, and Self-Motivation. I am capable of working independently and eager to learn new knowledge, even willing to adopt a doctoral student mindset. I aim to contribute to highly original research. Additionally, I am open to discussing ideas with group members and assisting in guiding PhD students. Computational Resources. Due to my professional experience in the High Performance Computer Research Center, I have established good collaborative relationships with some leading supercomputing companies, internet companies, as well as distributed training centers in universities and research institutes. If necessary, I can provide the required computational resources for our group. Relevant Theoretical Foundation. I obtained my Ph.D. in the School of Mathematical Sciences, with strong algorithm analysis skills and the ability to learn new theories. My background covers deep learning optimizers, reinforcement learning, stochastic analysis, control and game theory, and PDEs. In addition, almost all my personal information (referees, publications, and etc.) related to an postdoctoral application can be found on this website. If you have any interest and question, please feel free to contact me or refer to the FAQ section.\n","permalink":"https://zhaotong94.github.io/news/my-first-news/","summary":"I would like to apply for a postdoctoral position in the AI-related field. I have a multidisciplinary background in mathematics, AI, high-performance computing and following advantages:\nCuriosity, Adaptability, and Self-Motivation. I am capable of working independently and eager to learn new knowledge, even willing to adopt a doctoral student mindset. I aim to contribute to highly original research. Additionally, I am open to discussing ideas with group members and assisting in guiding PhD students. Computational Resources. Due to my professional experience in the High Performance Computer Research Center, I have established good collaborative relationships with some leading supercomputing companies, internet companies, as well as distributed training centers in universities and research institutes. If necessary, I can provide the required computational resources for our group. Relevant Theoretical Foundation. I obtained my Ph.D. in the School of Mathematical Sciences, with strong algorithm analysis skills and the ability to learn new theories.","title":"My First News"},{"content":"Motivation When giving a presentation on STEM disciplines, we usually need to draw a gif of some dynamical systems to express our ideas clearly. However, creating a portable, fluid, and authentic gif is challenging, it is very hard to find suitable parameters for drawing a gif.\nChallenge No matter how long this process takes, we only have a limited time to watch (usually very short). How to define fluidity of an animation (comfortable to watch)? Due to the stability of the discretize dynamical system of interest, we can not arbitrarily choose a stepsize for the system. What is the relationship between the size of the created gif and fluidity? Analysis We define three concept for $i\\in\\{r,o\\}$: $$\r\\begin{align}\rD_i:\u0026=\\frac{\\Delta_i}{F_i},\\label{d}\\\\\rR_i:\u0026=\\frac{F_i}{T_i},\\\\\rS_i:\u0026=\\frac{R_i}{D_i},\\\\\rv_i:\u0026=D_i R_i, \\label{v}\r\\end{align}\r$$ where $r$ and $o$ represent the above quantities is measured in reality and observation respectively, $\\Delta_i$ is the total variation, or the total path length of things moving in the image measured by some reasonable metrics, $F_i$ is total number of images, $T_i$ is total time, $D_i$ is resolution (the average difference between two adjacent images in the series), $R_i$ is frame rate, $S_i$ is fluidity or smoothness of a gif, and $v_i$ is the velocity of things in images.\nIn our creating-a-gif-for-dynamical-systems case, $R_r$ is how many images are calculated per unit real time. Considering $$\r\\begin{align*}\r\\dot{\\theta}_t\u0026=-L^{\\prime}(t,\\theta_t),\\\\\r\\theta_{t+1} \u0026=\\theta_t-\\eta L^{\\prime}(t,\\theta_t),\r\\end{align*}\r$$ where $\\eta$ is learning rate. At every time step $t$, an image $\\theta_{t}$ is generated, and therefore $R_r=\\frac{1}{\\eta}$.\nIn order to guarantee the stability of the dynamical system, we have to increase $R_r$. However, if we save all these easy-to-calculate images into gif it is usually lead to prohibitively large gif file. So, we need to sample some images with compression ratio $k=\\frac{F_r}{F_o}$ and store them into gif. To keep the gif authentic (without distortion), we set $l=\\frac{T_r}{T_o}$, acceleration ratio, a constant (otherwise the process distorted) and therefore $$\r\\begin{align*}\rk \\eta\u0026=\\frac{l}{R_o},\\\\\r\\end{align*} $$ where $R_o$ is the FPS of gif (a constant). According to equations \\eqref{d}-\\eqref{v}, we have $$\r\\begin{align*}\rS_o\u0026=\\frac{R_o}{D_o}=\\frac{F^2_o}{T_o\\Delta}=\\frac{F^2_o}{T_o T_r v_r},\\\\\rF_o\u0026=\\frac{R_r T_r}{k}, \\end{align*}\r$$ where $\\Delta=\\Delta_r=\\Delta_o$ in this case, $F_o$ is proportional to the size of the gif file. Finally, we get the suitable compression ratio $k$ and corresponding gif size measured by $F_o$ $$\r\\begin{align*}\rk\u0026=\\frac{\\sqrt{T_r}}{\\eta\\sqrt{S_o T_o v_r}}, \\; v_r=|L^{\\prime}|,\\\\\rF_o\u0026=\\sqrt{S_o T_o T_r v_r}.\r\\end{align*}\r$$ ","permalink":"https://zhaotong94.github.io/blog/hcssdgds/","summary":"Motivation When giving a presentation on STEM disciplines, we usually need to draw a gif of some dynamical systems to express our ideas clearly. However, creating a portable, fluid, and authentic gif is challenging, it is very hard to find suitable parameters for drawing a gif. Challenge No matter how long this process takes, we only have a limited time to watch (usually very short). How to define fluidity of an animation (comfortable to watch)? Due to the stability of the discretize dynamical system of interest, we can not arbitrarily choose a stepsize for the system. What is the relationship between the size of the created gif and fluidity? Analysis We define three concept for $i\\in\\{r,o\\}$: $$ \\begin{align} D_i:\u0026=\\frac{\\Delta_i}{F_i},\\label{d}\\\\ R_i:\u0026=\\frac{F_i}{T_i},\\\\ S_i:\u0026=\\frac{R_i}{D_i},\\\\ v_i:\u0026=D_i R_i, \\label{v} \\end{align} $$ where $r$ and $o$ represent the above quantities is measured in reality and observation","title":"How to create a small and smooth dynamic GIF for dynamical systems?"},{"content":"ä»£ç ç”Ÿæˆç®—æ³• pip install latexify-py import latexify\nè¡¨æ ¼ç”Ÿæˆ table\nå›¾åƒç”Ÿæˆ draw\n","permalink":"https://zhaotong94.github.io/tech/paper_machine/","summary":"ä»£ç ç”Ÿæˆç®—æ³• pip install latexify-py import latexify è¡¨æ ¼ç”Ÿæˆ table å›¾åƒç”Ÿæˆ draw","title":"Paper_machine"},{"content":"","permalink":"https://zhaotong94.github.io/tech/latex_cmd/","summary":"","title":"Latex_cmd"},{"content":"ç”¨æˆ·ç›®å½•ä¸‹è½½anaconda\ncurl anacoda å»ºç«‹ç¯å¢ƒ\nconda create --name \u0026lt;env_name\u0026gt; python=3.9 æ¿€æ´»ç¯å¢ƒ\nconda activate \u0026lt;env_name\u0026gt; å®‰è£…pytorchï¼Œå®˜ç½‘å¯»æ‰¾å‘½ä»¤ï¼Œæ³¨æ„ä¸è‡ªå·±çš„cudaåŒ¹é…\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 åˆ é™¤condaç¯å¢ƒ\nconda env remove --name \u0026lt;env_name\u0026gt; æŸ¥çœ‹æ‰€æœ‰æ¨¡å—\nmodule av åŠ è½½æŒ‡å®šæ¨¡å—\nmodule load \u0026lt;module_name\u0026gt; æ·»åŠ ç¯å¢ƒå˜é‡\nexport PATH=\u0026lt;path_to_dir\u0026gt;:$PATH åˆ—å‡ºå½“å‰å·²åŠ è½½æ¨¡å—\nmodule list æŸ¥è¯¢æ˜¯å“ªä¸ªå¯æ‰§è¡ŒäºŒè¿›åˆ¶æ–‡ä»¶\nwhich xxx ","permalink":"https://zhaotong94.github.io/tech/build_ai_env/","summary":"ç”¨æˆ·ç›®å½•ä¸‹è½½anaconda curl anacoda å»ºç«‹ç¯å¢ƒ conda create --name \u0026lt;env_name\u0026gt; python=3.9 æ¿€æ´»ç¯å¢ƒ conda activate \u0026lt;env_name\u0026gt; å®‰è£…pytorchï¼Œå®˜ç½‘å¯»æ‰¾å‘½ä»¤ï¼Œæ³¨æ„ä¸è‡ªå·±çš„cudaåŒ¹é… pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 åˆ é™¤condaç¯å¢ƒ conda env remove --name \u0026lt;env_name\u0026gt; æŸ¥çœ‹æ‰€æœ‰æ¨¡å— module av åŠ è½½æŒ‡å®šæ¨¡å— module load \u0026lt;module_name\u0026gt; æ·»åŠ ç¯å¢ƒå˜é‡ export PATH=\u0026lt;path_to_dir\u0026gt;:$PATH åˆ—å‡ºå½“å‰å·²åŠ è½½æ¨¡å— module list æŸ¥è¯¢æ˜¯å“ªä¸ªå¯æ‰§è¡ŒäºŒè¿›åˆ¶æ–‡ä»¶ which xxx","title":"Build_AI_env"},{"content":"ä¸‹è½½æ–‡ä»¶\nwget -O url_link æˆ–è€…\ncurl -O url_link ä½¿ç”¨ä»£ç†\nproxychains4 wget -O url_link æˆ–è€…\nproxychains4 curl -O url_link proxychains4 ä½¿ç”¨sshä»å½“å‰è®¡ç®—æœºä¼ é€’å½“å‰æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶åˆ°è¿œç¨‹è®¡ç®—æœºçš„æŒ‡å®šç›®å½•\nscp -r ./* ./.??* user_name@IP_remote:~/path2traget_dir ","permalink":"https://zhaotong94.github.io/tech/remote_cmd/","summary":"ä¸‹è½½æ–‡ä»¶ wget -O url_link æˆ–è€… curl -O url_link ä½¿ç”¨ä»£ç† proxychains4 wget -O url_link æˆ–è€… proxychains4 curl -O url_link proxychains4 ä½¿ç”¨sshä»å½“å‰è®¡ç®—æœºä¼ é€’å½“å‰æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶åˆ°è¿œç¨‹è®¡ç®—æœºçš„æŒ‡å®šç›®å½• scp -r ./* ./.??* user_name@IP_remote:~/path2traget_dir","title":"Remote_cmd"},{"content":"æ‰“å¼€æ–‡ä»¶ï¼švim \u0026lt;file_name\u0026gt;\nç¼–è¾‘æ¨¡å¼ï¼ši\né€€å‡ºç¼–è¾‘æ¨¡å¼ï¼šesc\nä¿å­˜å¹¶é€€å‡ºï¼š:x\nå¤åˆ¶ï¼šéç¼–è¾‘æ¨¡å¼ä¸‹ï¼Œå…‰æ ‡èµ·å§‹ä½Vï¼Œå…‰æ ‡ç»“æŸä½yã€‚ ç²˜è´´ï¼šéç¼–è¾‘æ¨¡å¼ä¸‹ï¼Œå…‰æ ‡ä½åp, å…‰æ ‡ä½å‰Pã€‚\n","permalink":"https://zhaotong94.github.io/tech/vim_cmd/","summary":"æ‰“å¼€æ–‡ä»¶ï¼švim \u0026lt;file_name\u0026gt; ç¼–è¾‘æ¨¡å¼ï¼ši é€€å‡ºç¼–è¾‘æ¨¡å¼ï¼šesc ä¿å­˜å¹¶é€€å‡ºï¼š:x å¤åˆ¶ï¼šéç¼–è¾‘æ¨¡å¼ä¸‹ï¼Œå…‰æ ‡èµ·å§‹ä½Vï¼Œå…‰æ ‡ç»“æŸä½yã€‚ ç²˜è´´ï¼šéç¼–è¾‘æ¨¡å¼ä¸‹ï¼Œå…‰æ ‡ä½åp, å…‰æ ‡ä½å‰Pã€‚","title":"Vim_cmd"},{"content":"æäº¤ä»»åŠ¡\nsbatch \u0026lt;file_name.sh\u0026gt; æŸ¥çœ‹æäº¤é˜Ÿåˆ—ä¿¡æ¯\nsqueue \u0026lt;file_name.sh\u0026gt; æŸ¥çœ‹è¯¦ç»†æäº¤é˜Ÿåˆ—ä¿¡æ¯\nsqueue -o \u0026#34;%.18i %.9P %.10j %.8u %.8T %.10M %.9l %.6D %N %.20b\u0026#34; ç™»å½•å¹¶ç‹¬å æŸä¸€èŠ‚ç‚¹ï¼Œå®è¡Œäº¤äº’å¼ä½œä¸š\nsalloc -p your_partition_name -N1 ","permalink":"https://zhaotong94.github.io/tech/slurm_cmd/","summary":"æäº¤ä»»åŠ¡ sbatch \u0026lt;file_name.sh\u0026gt; æŸ¥çœ‹æäº¤é˜Ÿåˆ—ä¿¡æ¯ squeue \u0026lt;file_name.sh\u0026gt; æŸ¥çœ‹è¯¦ç»†æäº¤é˜Ÿåˆ—ä¿¡æ¯ squeue -o \u0026#34;%.18i %.9P %.10j %.8u %.8T %.10M %.9l %.6D %N %.20b\u0026#34; ç™»å½•å¹¶ç‹¬å æŸä¸€èŠ‚ç‚¹ï¼Œå®è¡Œäº¤äº’å¼ä½œä¸š salloc -p your_partition_name -N1","title":"Slurm_cmd"},{"content":"æ–°å»ºæœ¬åœ°ä»“åº“\ngit init æ·»åŠ åˆ°æš‚å­˜åŒº\ngit add . æ·»åŠ åˆ°ä»“å‚¨åŒº\ngit commit -m \u0026#34;content for labeling this submission\u0026#34; åˆ é™¤\ngit remove åˆ é™¤ä»“åº“\ngit remove åˆ é™¤åˆ†æ”¯\ngit remove æ–°å»ºåˆ†æ”¯\ngit branch æŸ¥çœ‹åˆ†æ”¯\ngit branch åˆ‡æ¢åˆ†æ”¯\ngit switch é‡æ–°å‘½ååˆ†æ”¯\ngit è¿æ¥è¿œç¨‹ï¼ˆgithubï¼‰åˆ†æ”¯\ngit remote add åˆ›å»ºsshå¯†é’¥\nssh key-gen ","permalink":"https://zhaotong94.github.io/tech/git_cmd/","summary":"æ–°å»ºæœ¬åœ°ä»“åº“ git init æ·»åŠ åˆ°æš‚å­˜åŒº git add . æ·»åŠ åˆ°ä»“å‚¨åŒº git commit -m \u0026#34;content for labeling this submission\u0026#34; åˆ é™¤ git remove åˆ é™¤ä»“åº“ git remove åˆ é™¤åˆ†æ”¯ git remove æ–°å»ºåˆ†æ”¯ git branch æŸ¥çœ‹åˆ†æ”¯ git branch åˆ‡æ¢åˆ†æ”¯ git switch é‡æ–°å‘½ååˆ†æ”¯ git è¿æ¥è¿œç¨‹ï¼ˆgithubï¼‰åˆ†æ”¯ git remote add åˆ›å»ºsshå¯†é’¥ ssh key-gen","title":"Git_cmd"},{"content":"\u0026lt;div\u0026gt; ç§‘æŠ€ä»£ç  ç§‘æŠ€ä»£ç  ç§‘æŠ€ä»£ç  \u0026lt;/div\u0026gt; 1. ä»‹ç» scanå‘½ä»¤çš„ä½œç”¨å’Œkeys *çš„ä½œç”¨ç±»ä¼¼ï¼Œä¸»è¦ç”¨äºæŸ¥æ‰¾redisä¸­çš„é”®ï¼Œä½†æ˜¯åœ¨æ­£å¼çš„ç”Ÿäº§ç¯å¢ƒä¸­ä¸€èˆ¬ä¸ä¼šç›´æ¥ä½¿ç”¨keys *è¿™ä¸ªå‘½ä»¤ï¼Œå› ä¸ºä»–ä¼šè¿”å›æ‰€æœ‰çš„é”®ï¼Œå¦‚æœé”®çš„æ•°é‡å¾ˆå¤šä¼šå¯¼è‡´æŸ¥è¯¢æ—¶é—´å¾ˆé•¿ï¼Œè¿›è€Œå¯¼è‡´æœåŠ¡å™¨é˜»å¡ï¼Œæ‰€ä»¥éœ€è¦scanæ¥è¿›è¡Œæ›´ç»†è‡´çš„æŸ¥æ‰¾\nscanæ€»å…±æœ‰è¿™å‡ ç§å‘½ä»¤ï¼šscanã€sscanã€hscanã€zscanï¼Œåˆ†åˆ«ç”¨äºè¿­ä»£æ•°æ®åº“ä¸­çš„ï¼šæ•°æ®åº“ä¸­æ‰€æœ‰é”®ã€é›†åˆé”®ã€å“ˆå¸Œé”®ã€æœ‰åºé›†åˆé”®ï¼Œå‘½ä»¤å…·ä½“ç»“æ„å¦‚ä¸‹ï¼š\nscan cursor [MATCH pattern] [COUNT count] [TYPE type] sscan key cursor [MATCH pattern] [COUNT count] hscan key cursor [MATCH pattern] [COUNT count] zscan key cursor [MATCH pattern] [COUNT count] 2. scan scan cursor [MATCH pattern] [COUNT count] [TYPE type]ï¼Œcursorè¡¨ç¤ºæ¸¸æ ‡ï¼ŒæŒ‡æŸ¥è¯¢å¼€å§‹çš„ä½ç½®ï¼Œcounté»˜è®¤ä¸º10ï¼ŒæŸ¥è¯¢å®Œåä¼šè¿”å›ä¸‹ä¸€ä¸ªå¼€å§‹çš„æ¸¸æ ‡ï¼Œå½“è¿”å›0çš„æ—¶å€™è¡¨ç¤ºæ‰€æœ‰é”®æŸ¥è¯¢å®Œäº†\n127.0.0.1:6379[2]\u0026gt; scan 0 1) \u0026#34;3\u0026#34; 2) 1) \u0026#34;mystring\u0026#34; 2) \u0026#34;myzadd\u0026#34; 3) \u0026#34;myhset\u0026#34; 4) \u0026#34;mylist\u0026#34; 5) \u0026#34;myset2\u0026#34; 6) \u0026#34;myset1\u0026#34; 7) \u0026#34;mystring1\u0026#34; 8) \u0026#34;mystring3\u0026#34; 9) \u0026#34;mystring4\u0026#34; 10) \u0026#34;myset\u0026#34; 127.0.0.1:6379[2]\u0026gt; scan 3 1) \u0026#34;0\u0026#34; 2) 1) \u0026#34;myzadd1\u0026#34; 2) \u0026#34;mystring2\u0026#34; 3) \u0026#34;mylist2\u0026#34; 4) \u0026#34;myhset1\u0026#34; 5) \u0026#34;mylist1\u0026#34; $$\r\\begin{enumerate}\r\\item asfd\r\\item adf\r\\end{enumerate}\r$$ ","permalink":"https://zhaotong94.github.io/tech/tech1/","summary":"\u0026lt;div\u0026gt; ç§‘æŠ€ä»£ç  ç§‘æŠ€ä»£ç  ç§‘æŠ€ä»£ç  \u0026lt;/div\u0026gt; 1. ä»‹ç» scanå‘½ä»¤çš„ä½œç”¨å’Œkeys *çš„ä½œç”¨ç±»ä¼¼ï¼Œä¸»è¦ç”¨äºæŸ¥æ‰¾redisä¸­çš„é”®ï¼Œä½†æ˜¯åœ¨æ­£å¼çš„ç”Ÿäº§ç¯å¢ƒä¸­ä¸€èˆ¬ä¸ä¼šç›´æ¥ä½¿ç”¨keys *è¿™ä¸ªå‘½ä»¤ï¼Œå› ä¸ºä»–ä¼šè¿”å›æ‰€æœ‰çš„é”®ï¼Œå¦‚æœé”®çš„æ•°é‡å¾ˆå¤šä¼šå¯¼è‡´æŸ¥è¯¢æ—¶é—´å¾ˆé•¿ï¼Œè¿›è€Œå¯¼è‡´æœåŠ¡å™¨é˜»å¡ï¼Œæ‰€ä»¥éœ€è¦scanæ¥è¿›è¡Œæ›´ç»†è‡´çš„æŸ¥æ‰¾ sc","title":"Redis scan"},{"content":"Motivation The direct motivation of RKHS is to extend the following concepts and properties in $\\mathbb{R}^n$\nthe concept of evaluation functional $L_i$ at point $i\\in[n]$ in $\\mathbb{R}^n$ $$\rL_{i}:x\\mapsto \\left\\langle \\delta_i, x\\right\\rangle=x_i, \\forall x\\in\\mathbb{R}^n,\r$$ where $\\left\\langle \\cdot, \\cdot\\right\\rangle$ is linear functional notation and $\\delta_i(\\cdot)$ is Dirac function with mass at $i$, the eigenvector $e_i:=(0,0,\\cdots,1,\\cdots,0)$ in $\\mathbb{R}^n$ such that $$\r\\left\\langle \\delta_i, x\\right\\rangle=\\left\\langle e_i, x\\right\\rangle_{\\mathbb{R}^n}, \\forall x\\in\\mathbb{R}^n,\r$$ and more generally there exists $k_i$ in Hibert space with inner product $\\left\\langle \\cdot, \\cdot\\right\\rangle_H$ such that $$\r\\left\\langle \\delta_i, x\\right\\rangle=\\left\\langle k_i, x\\right\\rangle_{H}, \\forall x\\in\\mathbb{R}^n,\r$$ the property of expressing inner product $\\left\\langle x, y\\right\\rangle_H$ in $\\mathbb{R}^n$ as a production $$\rx^{\\textsf{T}}Hy, \\forall x,y\\in\\mathbb{R}^n,\r$$ the property that there exists an orthonormal basis $\\{e_1,e_2,\\cdots,e_n\\}$ bases in $ \\mathbb{R}^n $ such that the above positive definite matrix can be denoted as a diagonal matrix $$\rU=(e_1,e_2,\\cdots,e_n),\rU^{\\textsf{T}}HU=\\text{diag}(\\lambda_1,\\lambda_2,\\cdots,\\lambda_n)=\\sum_{i=1}^n\\lambda_ie_ie_i^\\textsf{T},\r$$ to corresponding ones in infinite dimensional linear space $\\mathcal{B}$ like followings, and we hope: an evalutation functional $L_x$ at point $x\\in\\mathcal{X}$ in infinite dimensional linear space $\\mathcal{B}(\\mathcal{X})$ of functions on set $\\mathcal{X}$ (usually card$(\\mathcal{X})\\geq\\aleph_0$) can be defined as $$\rL_{x}:f\\mapsto \\left\\langle \\delta_x, f\\right\\rangle=f(x), \\forall x\\in\\mathcal{X}, f\\in\\mathcal{B},\r$$ the eigenvector $\\delta_x(\\cdot)$ in $\\mathbb{L}^2(\\mathcal{X},\\mu)$ such that $$\r\\left\\langle \\delta_x, f\\right\\rangle=\\left\\langle \\delta_x, f\\right\\rangle_{\\mathbb{L}^2}, \\forall f\\in\\mathbb{L}^2(\\mathcal{X},\\mu),\r$$ where $\\mu$ is a measure, and more generally there exists $K_x(\\cdot)\\in\\mathcal{H}$ can be defined as $$\r\\left\\langle \\delta_x, f\\right\\rangle=\\left\\langle K_x, f\\right\\rangle_{\\mathcal{H}}, \\forall f\\in\\mathcal{H},\r$$ the inner product in Hilbert space $\\mathcal{H}$ can be defined as a binary integral form $$\r\\left\\langle f, g\\right\\rangle_\\mathcal{H}=\\int_{\\mathcal{X}\\times\\mathcal{X}} f(x)h(x,y)g(y)dxdy, \\forall x,y \\in\\mathcal{X}, f,g \\in\\mathcal{H},\r$$ there exists a set of orthonormal basis $\\phi(\\cdot,z)$ such that the inner product in a Hilbert space can be expressed in a \u0026ldquo;diagonal matrix\u0026rdquo; form, $$\r\\begin{align*}\r\\phi(z,w)^{\\textsf{T}}\u0026=\\phi(w,z),\\int_{\\mathcal{X}\\times\\mathcal{X}} \\phi^{\\textsf{T}}(z,w)h(w,z)\\phi(z,y)dwdz\\\\\r\u0026=\\int_{\\mathcal{X}\\times\\mathcal{X}} \\sigma(z)\\phi(x,z)\\phi(y,z)dz\\\\\r\u0026=\\sigma(y)\\delta(x-y).\r\\end{align*}\r$$ Conditions, Problems, and Solutions It is obviously that $L_x$ exists. Due to the motivation involves inner product, $\\mathcal{B}$ must be a Hilbert space $\\mathcal{B}:=\\mathcal{H}$.\nTo express the evaluation functional $L_x$ in the form of an inner product, we need to require that the linear functional is continuous (i.e. $sup_{f\\in\\mathcal{H}}\\frac{|L_x(f)|}{\\|f\\|_\\mathcal{H}}\u003c\\infty, \\forall x\\in\\mathcal{X}$), since the inner product must be continuous (i.e.$\\left\\langle f, g\\right\\rangle_\\mathcal{H}\\leq{\\|f\\|_\\mathcal{H}}{\\|g\\|_\\mathcal{H}}$). Then, the Riesz representation theorem implies that there exists a unique elements $K_x\\in\\mathcal{H}$ called reproducing kernel such that $\\left\\langle K_x, f\\right\\rangle_{\\mathcal{H}}=f(x),\\forall f\\in \\mathcal{H}$. Therefore, $\\left\\langle K_x, K_y\\right\\rangle_{\\mathcal{H}}=K_y(x):=K(y,x)$. Due to the sysmetry of inner product, $K(x,y)=K(y,x)$. In addition, $$\r\\mathcal{H}_0:=\\{f|f\\in\\overline{\\text{span}}\\{K_x(\\cdot)|x\\in{\\mathcal{X}}\\},\\left\\langle f, f\\right\\rangle_{\\mathcal{H}}\u003c\\infty\\}=\\mathcal{H}.\r$$ Considering Due to the motivation involves integral on $\\mathcal{X}$, we must introduce non-negative measure $\\mu$ (the sign is put into coefficient $f_K$) and define $$f:=\\int f_K(x)K(x,\\cdot)\\mu(dx)=\\sum_{i=1}^{\\infty}f_K(i)K(x_i,\\cdot)\u003c\\infty.$$ where $\\{K(x,\\cdot)|x\\in\\mathcal{X}, s.t. K(x,\\cdot)\\text{s} \\; \\text{form a basis with}\\;x\\;\\text{as less as possible}\\}=E$ is a basis set, since Hilbert space is complete (any point in it can be approximated by finite linear combination of basis). If $\\mathcal{H}$ is separable (e.g. $\\mathbb{L}^2(\\mathbb{R})$ and $\\mathbb{L}^2([0,1])$ with orthonormal basis Hermite polynomials $H_n(x)$ times $e^{-x^2/2}$ and Legendre polynomials), $\\text{card}(E)\\leq\\aleph_0$. If $\\text{card}(E)\u003e\\aleph_0$, we can choose a countable subset $E_0\\subset E$ to define $f$.\nOf course, we have $$\r\\begin{align*}\r\\|f\\|^2_\\mathcal{H}\u0026=\\left\\langle\\int f_K(x)K(x,\\cdot)\\mu(dx),\\int f_K(y)K(y,\\cdot)\\mu(dy)\\right\\rangle_{\\mathcal{H}}\\\\\r\u0026=\\int f_K(x)K(x,y)f_K(y)\\mu(dx)\\mu(dy)\\\\\r\u0026=\\int f(x)f_K(x)\\mu(dx)\u003c\\infty.\r\\end{align*}\r$$ The inner product is $$\r\\begin{align*}\r\\left\\langle f, g\\right\\rangle_\\mathcal{H}\u0026=\\left\\langle\\int f_K(x)K(x,\\cdot)\\mu(dx),\\int g_K(y)K(y,\\cdot)\\mu(dy)\\right\\rangle_{\\mathcal{H}}\\\\\r\u0026=\\int f_K(x)K(x,y)g_K(y)\\mu(dx)\\mu(dy)\u003c\\infty.\r\\end{align*}\r$$ For any generaly linearly independent basis set $E$ of $\\mathcal{H}$, and measure $\\mu_E$ on $E$, it behaves like in finite dimensional case $$\r\\left\\langle f, g\\right\\rangle_\\mathcal{H}=\\int_E f_E(e_x)H_E(e_x,e_y)g_E(e_y)\\mu_E(de_x)\\mu_E(de_y),\r$$ where $H_E$ is the representation of $\\left\\langle \\cdot, \\cdot \\right\\rangle_\\mathcal{H}$ and $f_E$ is the representation (coefficient) of $f$ under $E$ ($H_E(e_x,e_y):=\\left\\langle e_x, e_y\\right\\rangle_\\mathcal{H}, e_x(\\cdot), e_y(\\cdot)\\in E, f=\\int_E f_E(e_x)e_x(\\cdot)\\mu_E(de_x)$). If we can identify $(E,\\mu_E)$ with $(\\mathcal{x},\\mu) $, we have $$\r\\left\\langle f, g\\right\\rangle_\\mathcal{H}=\\int_\\mathcal{X} f_E(x)H_E(x,y)g_E(y)\\mu(dx)\\mu(dy).\r$$ For general cases, the measure $\\mu_E$ or $\\mu$ is count measure. Using Zorn\u0026rsquo;s Lemma, we know there exist a orthonormal basis $E$ of $\\mathcal{H}$, under which $$\r\\left\\langle f, g\\right\\rangle_\\mathcal{H}=\\int_E f_E(e_x)g_E(e_x)\\mu_E(de_x).\r$$ Fortunately, card($\\{K_x(\\cdot)\\})=\\aleph_0$ usually holds, but what is the condition required. Of course, $\\mathcal{H}$ is separable is a sufficient condition, but not pragmatic. In fact, if we regard it as a non-negative definite matrix $K(x,y)$ we hope we can obtain \\begin{equation*} K(x,y)=\\sum_{i=1}^{\\infty}\\sigma_i\\phi_i(x)\\phi_i(y). \\end{equation*} Luckily, the spectral theory guarantees the above equation holds, if $K$ is a compact operator. According to the requirement of spectral theory, one of following conditions should be satisfied:\nwe let $\\mu$ is $\\sigma$-finite measure, $K$ is squared-integrable on $\\mu\\otimes\\mu$, and then we find $T_K:\\mathbb{L}^{2}(\\mathcal{X})\\to \\mathbb{L}^{2}(\\mathcal{X})$, $$[T_{K}f](\\cdot)=\\int _{\\mathcal{X}}K(\\cdot,x)f(x)\\,d\\mu (x)$$ is a Hilbertâ€“Schmidt operator (compact operator). Mercer\u0026rsquo;s theorem. we let $K$ is continuous, $\\mathcal{X}$ is compact, $\\mu$ is positive finite Borel measure, and then we get $T_K:\\mathbb{L}^{2}(\\mathcal{X})\\to \\mathbb{L}^{2}(\\mathcal{X})$ as $$[T_{K}f](\\cdot)=\\int _{\\mathcal{X}}K(\\cdot,x)f(x)\\,d\\mu (x)$$ is compact operator. In both cases, $K(x, \\cdot)\\in \\mathbb{L}^{2}(\\mathcal{X},\\mu),\\;\\forall x\\in\\mathcal{X}$. $K(x,y)$ can be regard as inner product \u0026lsquo;martix\u0026rsquo; in both $\\mathbb{L}^{2}(\\mathcal{X},\\mu)$ space $(\\left\\langle e_x, e_y\\right\\rangle_{\\mathbb{L}^{2}})$ and $\\mathcal{H}$ space $(\\left\\langle K_x, K_y\\right\\rangle_\\mathcal{H})$. From $\\mathbb{L}^{2}(\\mathcal{X},\\mu)$ space perspective, $$\r\\begin{align}\rK(x,y)\u0026=\\sum_{i=1}^{\\infty}\\sigma_i\\phi_i(x)\\phi_i(y).\\label{k}\r\\end{align}\r$$ From $\\mathcal{H}$ space perspective, under basis $$\rh_i(\\cdot)=\\int K(\\cdot,x)\\phi_i(x)\\mu(dx)=\\sigma_i\\phi_i(\\cdot),\r$$ inner product $\\left\\langle \\cdot, \\cdot\\right\\rangle_\\mathcal{H}$ can be written as diag($\\sigma_1,\\sigma_2,\\cdots$), hence $$\r\\left\\langle h_i, h_j\\right\\rangle_\\mathcal{H}=\\sigma_i\\sigma_j\\left\\langle \\phi_i, \\phi_j\\right\\rangle_\\mathcal{H}=\\sigma_i\\delta_{ij}=\\sigma_i\\left\\langle \\phi_i, \\phi_j\\right\\rangle_{\\mathbb{L}^{2}(\\mathcal{X})} ,\r$$ For case one, we have $\\sum_i \\sigma_i^2\u003c\\infty$ ($\\sigma_i\\to 0$), therefore Im$(T_{K})\\subset\\mathcal{H}\\subset\\mathbb{L}^{2}(\\mathcal{X})$. Conditions in case two is strictly stronger than that in one, it comes to the same conclusion as in case one. The difference is the decomposition of $K$ \\eqref{k} in case one converges in $\\mathbb{L}^{2}$ norm rather than uniformly in case two.\nRemark: Conditions on reproducing kernel $K(\\cdot, \\cdot)$ (integrable, continuous) embed $\\mathcal{H}$ into a countable dimensional subspace of $\\mathbb{R}^\\mathcal{X}$ without considering the topology on these two space. For any $h\\in\\mathcal{H}$, we can not evaluate $h(x)$ at $x$ randowly, as values at different points fulfill some constraints inherted from reproducing kernel $K$. The operator $T_K$ can be seen as a infinite dimensional version SVD decomposition. Due to bounded $\\sigma_i$, $T_K$ compresses and projects a point in $\\mathbb{L}^2$ to $\\mathcal{H}$. Therefore, $$\r\\left\\langle f, g\\right\\rangle_\\mathcal{H}=\\left\\langle \\sum_i\\left\\langle f, \\phi_i\\right\\rangle_{\\mathbb{L}^{2}}\\phi_i , \\sum_j\\left\\langle f, \\phi_j\\right\\rangle_{\\mathbb{L}^{2}}\\phi_j\\right\\rangle_\\mathcal{H}=\\sum_i\\frac{\\left\\langle f, \\phi_i\\right\\rangle_{\\mathbb{L}^{2}}\\left\\langle g, \\phi_i\\right\\rangle_{\\mathbb{L}^{2}}}{\\sigma_i}.\r$$ Besides, considering an embedding map $I:\\mathcal{H}\\to\\mathbb{L}^{2}$, we find the map is continuous (i.e.$\\|x-y\\|_{\\mathbb{L}^{2}}\\leq \\|I(x)-I(y)\\|_\\mathcal{H},\\; \\forall x,y\\in \\mathcal{H}$). Hence topologies ($\\mathcal{T}_{\\mathbb{L}^{2}}, \\mathcal{T}_\\mathcal{H}$) determinded by their metric have relationship $\\mathcal{T}_{\\mathbb{L}^{2}}\\subset\\mathcal{T}_\\mathcal{H}$.\nAll the conclusion can be applied to complex cases. Some values need to be written as their conjugates.\nExamples Gaussian linear polynomial ","permalink":"https://zhaotong94.github.io/blog/rkhs/","summary":"Motivation The direct motivation of RKHS is to extend the following concepts and properties in $\\mathbb{R}^n$ the concept of evaluation functional $L_i$ at point $i\\in[n]$ in $\\mathbb{R}^n$ $$ L_{i}:x\\mapsto \\left\\langle \\delta_i, x\\right\\rangle=x_i, \\forall x\\in\\mathbb{R}^n, $$ where $\\left\\langle \\cdot, \\cdot\\right\\rangle$ is linear functional notation and $\\delta_i(\\cdot)$ is Dirac function with mass at $i$, the eigenvector $e_i:=(0,0,\\cdots,1,\\cdots,0)$ in $\\mathbb{R}^n$ such that $$ \\left\\langle \\delta_i, x\\right\\rangle=\\left\\langle e_i, x\\right\\rangle_{\\mathbb{R}^n}, \\forall x\\in\\mathbb{R}^n, $$ and more generally there exists $k_i$ in Hibert space with inner product $\\left\\langle \\cdot, \\cdot\\right\\rangle_H$ such that $$ \\left\\langle \\delta_i, x\\right\\rangle=\\left\\langle k_i, x\\right\\rangle_{H}, \\forall x\\in\\mathbb{R}^n, $$ the property of expressing inner product $\\left\\langle x, y\\right\\rangle_H$ in $\\mathbb{R}^n$ as a production $$ x^{\\textsf{T}}Hy, \\forall x,y\\in\\mathbb{R}^n, $$ the property that there exists an orthonormal basis $\\{e_1,e_2,\\cdots,e_n\\}$ bases in $ \\mathbb{R}^n $ such that the above positive definite matrix can be denoted as a diagonal","title":"Reproducing Kernel Hilbert Space"},{"content":" é¦–é¡µåŠ comingï¼Œæœ€è¿‘æ›´æ–°, è¯­å½•ï¼Œæ·»åŠ æ–‡æœ«è®¨è®ºéƒ¨åˆ†ã€‚ æ·»åŠ codeï¼Œpaperï¼Œsuppï¼Œpptï¼Œvedioï¼Œdoiï¼Œabstractï¼Œblogï¼Œposterç­‰æ¨¡å—ã€‚ hugo markdownä½œå›¾ å¤–å¾®åˆ†ï¼Œè”ç»œï¼Œé»æ›¼å‡ ä½• å¼ºåŒ–å­¦ä¹ ï¼Œæ§åˆ¶ æœ€ä¼˜æ§åˆ¶è§’åº¦ç†è§£ddpm bsdeçš„ç†è§£ é©¬é‡Œä¸‡å¯¼æ•°çš„ç†è§£ å¸ƒå°”å·´åŸºå­¦æ´¾ èŒƒç•´ä¸å‡½å­ æ‹Ÿé‡åŠ›ä¼˜åŒ–å™¨ å› å¼åˆ†è§£ \u0026ldquo;\\Climb heights never reached, gain perspectives nerve realized, discover things never seen, and create worlds never expected. \\ \u0026ndash;Tong \\ Reading broadens perspectives and inspires thought; writing perceives nuance and clarifies logic.\\ \u0026ndash;Tong\u0026rdquo; ","permalink":"https://zhaotong94.github.io/tech/memo/","summary":"é¦–é¡µåŠ comingï¼Œæœ€è¿‘æ›´æ–°, è¯­å½•ï¼Œæ·»åŠ æ–‡æœ«è®¨è®ºéƒ¨åˆ†ã€‚ æ·»åŠ codeï¼Œpaperï¼Œsuppï¼Œpptï¼Œvedioï¼Œdoiï¼Œabstractï¼Œblogï¼Œposterç­‰æ¨¡å—ã€‚ hugo markdownä½œå›¾ å¤–å¾®åˆ†ï¼Œè”ç»œï¼Œé»æ›¼å‡ ä½• å¼ºåŒ–å­¦ä¹ ï¼Œæ§åˆ¶ æœ€ä¼˜æ§åˆ¶è§’åº¦ç†è§£ddpm bsdeçš„ç†è§£ é©¬é‡Œä¸‡å¯¼æ•°çš„ç†è§£ å¸ƒ","title":"Memo"},{"content":"Do you often encounter a shortage of letters when writing papers? The arbitrary use of letters earlier on can lead to conflicts when defining new variables later, and changing already defined letters can result in inevitable tedious modifications.\nHave you ever read papers where undefined letters appear and often affect understanding? It often requires repeatedly searching and wasting a lot of time, and more importantly, you may not necessarily find them.\nTo solve the above two problems, I have compiled a list of common letter usages in the field of computer science and mathematics (case insensitive), and this table will be updated periodically.\nA: action\nB: borel base,batch\nC: constant, channel\nD: data, degree, distance (metric), dimension\nE: edge, element, expectation\nF: filter, function, face\nG: graph, function\nH: Hilbert, function, height, Hessian\nI: dumb index, identifier, imaginary_unit\nJ: dumb index\nK: key, dumb index, constant\nL: length, operator\nM: matrix, measure\nN: positive_integer, total_number\nO: order\nP: permutationï¼Œprobability\nQ: query, rational_numerï¼Œprobability\nR: real, reward\nS: sample, set, state\nT: time, topology, transform, temperature\nU: variable, open set, domain\nV: variable, open set, range, vertex\nW: variable, width, weight\nX: input, unknown_number, variate\nY: output, unknown_number, variate\nZ: unknown_number, variate, integer\n$\\alpha$: constant\n$\\beta$: coefficient for EMA, costant\n$\\chi$: Chi-squared distribution, statistics, constant,\n$\\delta$: delta function, $\\epsilon$-$\\delta$ language\n$\\epsilon$: $\\epsilon$-$\\delta$ language\n$\\varepsilon$: $\\epsilon$-$\\delta$ language\n$\\phi$: function\n$\\gamma$: function, constant\n$\\eta$: learning_rate $\\iota$: including_map\n$\\kappa$: constant, $\\kappa$-coefficient\n$\\lambda$: eigenvalue, laplacian_factor\n$\\mu$: measure, distribution, mean\n$\\nu$: photon, constant\n$\\omicron$:infinitesimal\n$\\pi$: policy, distribution, measure\n$\\theta$: parameter\n$\\rho$: density\n$\\sigma$: sum, standard_deviation, variation\n$\\tau$: permutation, stopping time\n$\\upsilon$: displacement, constant\n$\\omega$: order, domain, open set\n$\\xi$: variate\n$\\psi$: function\n$\\zeta$: variate\nFor the first issue, when writing a paper and needing to define a new letter but without a particularly good choice, select letters whose common usage ï¼ˆlisted aboveï¼‰ do definitely not appear your paper for use.\nFor the second issue, when encountering undefined letters, directly check the above list to see if some common usage has a reasonable explanation within the paper.\nFor any republication, please contact me.\n","permalink":"https://zhaotong94.github.io/tech/tech/","summary":"Do you often encounter a shortage of letters when writing papers? The arbitrary use of letters earlier on can lead to conflicts when defining new variables later, and changing already defined letters can result in inevitable tedious modifications. Have you ever read papers where undefined letters appear and often affect understanding? It often requires repeatedly searching and wasting a lot of time, and more importantly, you may not necessarily find them. To solve the above two problems, I have compiled a list of common letter usages in the field of computer science and mathematics (case insensitive), and this table will be updated periodically. A: action B: borel base,batch C: constant, channel D: data, degree, distance (metric), dimension E: edge, element, expectation F: filter, function, face G: graph, function H: Hilbert, function, height, Hessian I: dumb index, identifier, imaginary_unit J: dumb","title":"Running out of letters? Unclear meanings in papers?"},{"content":"","permalink":"https://zhaotong94.github.io/cv/","summary":"","title":"ğŸ‘¨â€ğŸ¦±CV"},{"content":"@inproceedings{hu2023rlekf, abbr={AAAI}, title={RLEKF: An optimizer for deep potential with ab initio accuracy}, author={Hu*, Siyu and Zhang*, Wentao and Sha, Qiuchen and Pan, Feng and Wang, Lin-Wang and Jia, Weile and Tan, Guangming and Zhaoâ€ , Tong}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, volume={37}, number={7}, pages={7910â€“7918}, year={2023}, abstract={It is imperative to accelerate the training of neural network force field such as Deep Potential, which usually requires thousands of images based on first-principles calculation and a couple of days to generate an accurate potential energy surface. To this end, we propose a novel optimizer named reorganized layer extended Kalman filtering (RLEKF), an optimized version of global extended Kalman filtering (GEKF) with a strategy of splitting big and gathering small layers to overcome the $O(N^2)$ computational cost of GEKF. This strategy provides an approximation of the dense weights error covariance matrix with a sparse diagonal block matrix for GEKF. We implement both RLEKF and the baseline Adam in our Î±Dynamics package and numerical experiments are performed on 13 unbiased datasets. Overall, RLEKF converges faster with slightly better accuracy. For example, a test on a typical system, bulk copper, shows that RLEKF converges faster by both the number of training epochs (Ã—11.67) and wall-clock time (Ã—1.19). Besides, we theoretically prove that the updates of weights converge and thus are against the gradient exploding problem. Experimental results verify that RLEKF is not sensitive to the initialization of weights. The RLEKF sheds light on other AI-for-science applications where training a large neural network (with tons of thousands parameters) is a bottleneck.}, bibtex_show={true}, pdf={https://arxiv.org/pdf/2212.06989}, code={}, blog={}, poster={}, slides={}, video={}, url={}, grade={oral}, doi={https://doi.org/10.1609/aaai.v37i7.25957}, altmetric={248277}, dimensions={true}, google_scholar_id={}, selected={true}, publisher={}, }\n@inproceedings{feng2024accelerating, abbr={EuroPar}, bibtex_show={true}, title={Accelerating large-scale sparse LU factorization for RF circuit simulation}, abstract={Sparse LU factorization is the indispensable building block of the circuit simulation, and dominates the simulation time, especially when dealing with large-scale circuits. Radio frequency (RF) circuits have been increasingly emphasized with the evolution of ubiquitous wireless communication (i.e., 5G and WiFi). The RF simulation matrices show a distinctive pattern of structured dense blocks, and this pattern has been inadvertently overlooked by prior works, leading to the underutilization of computational resources. In this paper, by exploiting the block structure, we propose a novel blocked format for L and U factors and re-design the large-scale sparse LU factorization accordingly, which leverages the data locality inherent in RF matrices. The data format transformation is streamlined, strategically eliminating the redundant data movement and costly indirect memory access. Moreover, the vector operations are converted into matrix operations, enabling efficient data reuse and enhancing data-level parallelism. The experiment results demonstrate that our method achieves superior performance compared to state-of-the-art implementation.}, author={Feng, Guofeng and Wang, Hongyu and Guo, Zhuoqiang and Liâ€ , Mingzhen and Zhao, Tong and Jin, Zhou and Jia, Weile and Tan, Guangming and Sun, Ninghui}, booktitle={European Conference on Parallel Processing}, volume={}, number={}, pages={182â€“195}, year={2024}, publisher={Springer}, doi={https://doi.org/10.1007/978-3-031-69583-4_13}, }\n@inproceedings{hu2024training, abbr={PPoPP}, bibtex_show={true}, title={Training one deepmd model in minutes: A step towards online learning}, abstract={Neural Network Molecular Dynamics (NNMD) has become a major approach in material simulations, which can speedup the molecular dynamics (MD) simulation for thousands of times, while maintaining ab initio accuracy, thus has a potential to fundamentally change the paradigm of material simulations. However, there are two time-consuming bottlenecks of the NNMD developments. One is the data access of ab initio calculation results. The other, which is the focus of the current work, is reducing the training time of NNMD model. The training of NNMD model is different from most other neural network training because the atomic force (which is related to the gradient of the network) is an important physical property to be fit. Tests show the traditional stochastic gradient methods, like the Adam algorithms, cannot efficiently deploy the multisample minibatch algorithm. As a result, a typical training (taking the Deep Potential Molecular Dynamics (DeePMD) as an example) can take many hours. In this work, we designed a heuristic minibatch quasi-Newtonian optimizer based on Extended Kalman Filter method. An early reduction of gradient and error is adopted to reduce memory footprint and communication. The memory footprint, communication and settings of hyper-parameters of this new method are analyzed in detail. Computational innovations such as customized kernels of the symmetry-preserving descriptor are applied to exploit the computing power of the heterogeneous architecture. Experiments are performed on 8 different datasets representing different real case situations, and numerical results show that our new method has an average speedup of 32.2 compared to the Reorganized Layer-wised Extended Kalman Filter with 1 GPU, reducing the absolute training time of one DeePMD model from hours to several minutes, making it one step toward online training.}, author={Hu, Siyu and Zhao, Tong and Sha, Qiuchen and Li, Enji and Meng, Xiangyu and Liu, Lijun and Wang, Lin-Wang and Tan, Guangming and Jia, Weile}, booktitle={Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming}, pdf={https://dl.acm.org/doi/pdf/10.1145/3627535.3638505}, selected={true}, volume={}, number={}, pages={257â€“269}, year={2024}, publisher={}, }\n@inproceedings{li1950enhance, abbr={SC}, bibtex_show={true}, title={Enhance the strong scaling of lammps on fugaku}, abstract={Physical phenomenon such as protein folding requires simulation up to microseconds of physical time, which directly corresponds to the strong scaling of molecular dynamics(MD) on modern supercomputers. In this paper, we present a highly scalable implementation of the state-of-the-art MD code LAMMPS on Fugaku by exploiting the 6D mesh/torus topology of the TofuD network. Based on our detailed analysis of the MD communication pattern, we first adapt coarse-grained peer-to-peer ghost-region communication with uTofu interface, then further improve the scalability via fine-grained thread pool. Finally, Remote direct memory access (RDMA) primitives are utilized to avoid buffer overhead. Numerical results show that our optimized code can reduce 77% of the communication time, improving the performance of baseline LAMMPS by a factor of 2.9x and 2.2x for Lennard-Jones and embedded-atom method potentials when scaling to 36, 846 computing nodes. Our optimization techniques can also benefit other applications with stencil or domain decomposition methods.}, author={Li, Jianxiong and Zhao, Tong and Guo, Zuoqiang and Shi, Shunchen and Liu, Lijun and Tan, Guangming and Jiaâ€ , Weile and Yuan, Guojun and Wangâ€ , Zhan}, booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, pdf={https://dl.acm.org/doi/pdf/10.1145/3581784.3607064}, volume={}, number={}, pages={1-13}, year={2023}, publisher={Association for Computing Machinery}, }\n@article{yan190510-million, bibtex_show={true}, abbr={JCST}, title={10-million atoms simulation of first-principle package LS3DF}, author={Yan, Yujin and Liâ€ , HaiBo and Zhao, Tong and Wang, Lin-Wang and Shi, Lin and Liu, Tao and Tan, GuangMing and Jiaâ€ , Weile and Sunâ€ , Ninghui}, abstract={The growing demand for semiconductor devices simulation poses a big challenge for large-scale electronic structure calculations. Among various methods, the linearly scaling three-dimensional fragment (LS3DF) method exhibits excellent scalability in large-scale simulations. Based on algorithmic and system-level optimizations, we propose a highly scalable and highly efficient implementation of LS3DF on a domestic heterogeneous supercomputer equipped with accelerators. In terms of algorithmic optimizations, the original all-band conjugate gradient algorithm is refined to achieve faster convergence, and mixed precision computing is adopted to increase overall efficiency. In terms of system-level optimizations, the original two-layer parallel structure is replaced by a coarse-grained parallel method. Optimization strategies such as multi-stream, kernel fusion, and redundant computation removal are proposed to increase further utilization of the computational power provided by the heterogeneous machines. As a result, our optimized LS3DF can scale to a 10-million silicon atoms system, attaining a peak performance of 34.8 PFLOPS (21.2% of the peak). All the improvements can be adapted to the next-generation supercomputers for larger simulations.}, journal={Journal of Computer Science and Technology}, volume={39}, number={1}, pages={45â€“62}, year={2024}, doi={https://doi.org/10.1007/s11390-023-3011-6}, award={}, award_name={}, }\n@article{zhao2022limits, bibtex_show={true}, abbr={CAM}, title={Limits of one-dimensional interacting particle systems with two-scale interaction}, author={Zhao, Tong}, abstract={This paper characterizes the limits of a large system of interacting particles distributed on the real line. The interaction occurring among neighbors involves two kinds of independent actions with different rates. This system is a generalization of the voter process, of which each particle is of type A or a. Under suitable scaling, the local proportion functions of A particles converge to continuous functions which solve a class of stochastic partial differential equations driven by Fisher-Wright white noise. To obtain the convergence, the tightness of these functions is derived from the moment estimate method.}, journal={Chinese Annals of Mathematics, Series B}, volume={43}, number={2}, pages={195-208}, year={2022}, doi={https://doi.org/10.1007/s11401-022-0311-z}, selected={true}, award={}, award_name={}, }\n","permalink":"https://zhaotong94.github.io/publications/","summary":"@inproceedings{hu2023rlekf, abbr={AAAI}, title={RLEKF: An optimizer for deep potential with ab initio accuracy}, author={Hu*, Siyu and Zhang*, Wentao and Sha, Qiuchen and Pan, Feng and Wang, Lin-Wang and Jia, Weile and Tan, Guangming and Zhaoâ€ , Tong}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, volume={37}, number={7}, pages={7910â€“7918}, year={2023}, abstract={It is imperative to accelerate the training of neural network force field such as Deep Potential, which usually requires thousands of images based on first-principles calculation and a couple of days to generate an accurate potential energy surface. To this end, we propose a novel optimizer named reorganized layer extended Kalman filtering (RLEKF), an optimized version of global extended Kalman filtering (GEKF) with a strategy of splitting big and gathering small","title":"ğŸ“„Publications"},{"content":"Q: Why have you decided not to pursue a position as an assistant professor?/ A: There are three main reasons: first, I wish to engage in more creative research at institutions with higher research standards to satisfy my intellectual curiosity; second, I want to enrich my research and life experiences; and third, I am more passionate about artificial intelligence.// Q: What if I see something incorrect in the post?/ A: Please feel free to let me knowï¼Œand I will correct it.// Q: Can I repost or translate the blog?/ A: It is my pleasure, but please clearly indicate that it is a repost and pin this notice at the top.\n","permalink":"https://zhaotong94.github.io/faq/","summary":"Q: Why have you decided not to pursue a position as an assistant professor?/ A: There are three main reasons: first, I wish to engage in more creative research at institutions with higher research standards to satisfy my intellectual curiosity; second, I want to enrich my research and life experiences; and third, I am more passionate about artificial intelligence.// Q: What if I see something incorrect in the post?/ A: Please feel free to let me knowï¼Œand I will correct it.// Q: Can I repost or translate the blog?/ A: It is my pleasure, but please clearly indicate that it is a repost and pin this notice at the top.","title":"ğŸ¤”FAQ"}]